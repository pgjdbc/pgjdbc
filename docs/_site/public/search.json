[{"categories":null,"content":"This section describes the steps you need to take before you can write or run programs that use the JDBC interface.\nGetting the Driver    Precompiled versions of the driver can be downloaded from the PostgreSQL™ JDBC web site.\nAlternatively you can build the driver from source, but you should only need to do this if you are making changes to the source code. To build the JDBC driver, you need gradle and a JDK (currently at least jdk1.8).\nIf you have several Java compilers installed, maven will use the first one on the path. To use a different one set JAVA_HOME to the Java version you wish to use For example, to use a different JDK than the default, this may work:\nJAVA_HOME = /usr/local/jdk1.8.0_45 To compile the driver simply run gradlew assemble or gradlew build if you want to run the tests in the top level directory.\n NOTE\nIf you want to skip test execution, add the option -DskipTests. The compiled driver will be placed in pgjdbc/build/libs postgresql-MM.nn.pp.jar\n Where MM is the major version, nn is the minor version and pp is the patch version. Versions for JDBC3 and lower can be found here\nThis is a very brief outline of how to build the driver. Much more detailed information can be found on the github repo\nEven though the JDBC driver should be built with Gradle, for situations, where use of Gradle is not possible, e.g., when building pgJDBC for distributions, the pgJDBC Gradle build provides a convenience source release artifact *-src.tar.gz - a Maven based project. The Maven based project contains a version of the JDBC driver with complete functionality, which can be used in production and is still validly buildable within the Maven build environment. The Maven-based project is created with gradlew -d :postgresql:sourceDistribution -Prelease. The produced *-src.tar.gz can be then found in pgjdbc/build/distributions/ directory. JDBC driver can be built from the Maven-based project with mvn package or, when the tests are to be skipped, with mvn -DskipTests package. Prefactored *-src.tar.gz s are released in the Maven central repository.\nSetting up the Class Path    To use the driver, the JAR archive named postgresql-MM.nn.pp.jar needs to be included in the class path, either by putting it in the CLASSPATH environment variable, or by using flags on the java command line.\nFor instance, assume we have an application that uses the JDBC driver to access a database, and that application is installed as /usr/local/lib/myapp.jar . The PostgreSQL™ JDBC driver installed as /usr/local/pgsql/share/java/postgresql-MM.nn.pp.jar . To run the application, we would use:\nexport CLASSPATH=/usr/local/lib/myapp.jar:/usr/local/pgsql/share/java/postgresql-42.2.15.jar:. java MyApp Current Java applications will likely use maven, gradle or some other package manager. Use this to search for the latest jars and how to include them in your project\nLoading the driver from within the application is covered in Initializing the Driver.\nPreparing the Database Server for JDBC    Out of the box, Java does not support unix sockets so the PostgreSQL server must be configured to allow TCP/IP connections. Starting with server version 8.0 TCP/IP connections are allowed from localhost . To allow connections to other interfaces than the loopback interface, you must modify the postgresql.conf file’s listen_addresses setting.\nOnce you have made sure the server is correctly listening for TCP/IP connections the next step is to verify that users are allowed to connect to the server. Client authentication is setup in pg_hba.conf . Refer to the main PostgreSQL™ documentation for details .\nCreating a Database    When creating a database to be accessed via JDBC it is important to select an appropriate encoding for your data. Many other client interfaces do not care what data you send back and forth, and will allow you to do inappropriate things, but Java makes sure that your data is correctly encoded. Do not use a database that uses the SQL_ASCII encoding. This is not a real encoding and you will have problems the moment you store data in it that does not fit in the seven bit ASCII character set. If you do not know what your encoding will be or are otherwise unsure about what you will be storing the UNICODE encoding is a reasonable default to use.\n","description":"","title":"Setting up the JDBC Driver","uri":"/pgjdbc/documentation/setup/"},{"categories":null,"content":"This section describes how to load and initialize the JDBC driver in your programs.\nImporting JDBC    Any source that uses JDBC needs to import the java.sql package, using:\nimport java.sql.*;  NOTE\nYou should not import the org.postgresql package unless you are not using standard PostgreSQL™ extensions to the JDBC API.\n Loading the Driver    Applications do not need to explicitly load the org.postgresql. Driver class because the pgJDBC driver jar supports the Java Service Provider mechanism. The driver will be loaded by the JVM when the application connects to PostgreSQL™ (as long as the driver’s jar file is on the classpath).\n NOTE\nPrior to Java 1.6, the driver had to be loaded by the application - either by calling Class.forName(\"org.postgresql.Driver\"); or by passing the driver class name as a JVM parameter java -Djdbc.drivers=org.postgresql.Driver example.ImageViewer\n These older methods of loading the driver are still supported but they are no longer necessary.\nConnecting to the Database    With JDBC, a database is represented by a URL (Uniform Resource Locator). With PostgreSQL, this takes one of the following forms:\n jdbc:postgresql:database jdbc:postgresql:/ jdbc:postgresql://host/database jdbc:postgresql://host/ jdbc:postgresql://host:port/database jdbc:postgresql://host:port/  The parameters have the following meanings:\n  host = The host name of the server. Defaults to localhost . To specify an IPv6 address your must enclose the host parameter with square brackets, for example: jdbc:postgresql://[::1]:5740/accounting\n  port = The port number the server is listening on. Defaults to the PostgreSQL™ standard port number (5432).\n  database = The database name. The default is to connect to a database with the same name as the user name.\n  To connect, you need to get a Connection instance from JDBC. To do this, you use the DriverManager.getConnection() method: Connection db = DriverManager.getConnection(url, username, password)\nConnection Parameters    In addition to the standard connection parameters the driver supports a number of additional properties which can be used to specify additional driver behaviour specific to PostgreSQL™. These properties may be specified in either the connection URL or an additional Properties object parameter to DriverManager.getConnection . The following examples illustrate the use of both methods to establish a SSL connection.\nIf a property is specified both in URL and in Properties object, the value from Properties object is ignored.\nString url = \"jdbc:postgresql://localhost/test\"; Properties props = new Properties(); props.setProperty(\"user\", \"fred\"); props.setProperty(\"password\", \"secret\"); props.setProperty(\"ssl\", \"true\"); Connection conn = DriverManager.getConnection(url, props); String url = \"jdbc:postgresql://localhost/test?user=fred\u0026password=secret\u0026ssl=true\"; Connection conn = DriverManager.getConnection(url);   user = String The database user on whose behalf the connection is being made.\n  password = String The database user’s password.\n  options = String Specify ‘options’ connection initialization parameter. For example setting this to -c statement_timeout=5min would set the statement timeout parameter for this session to 5 minutes.\n  The value of this property may contain spaces or other special characters, and it should be properly encoded if provided in the connection URL. Spaces are considered to separate command-line arguments, unless escaped with a backslash ( \\ ); \\\\ represents a literal backslash.\nProperties props = new Properties(); props.setProperty(\"options\", \"-c search_path=test,public,pg_catalog -c statement_timeout=90000\"); Connection conn = DriverManager.getConnection(url, props); String url = \"jdbc:postgresql://localhost:5432/postgres?options=-c%20search_path=test,public,pg_catalog%20-c%20statement_timeout=90000\"; Connection conn = DriverManager.getConnection(url);   ssl (boolean)\nConnect using SSL. The server must have been compiled with SSL support. This property does not need a value associated with it. The mere presence of it specifies a SSL connection. However, for compatibility with future versions, the value “true” is preferred. For more information see Using SSL.\nSetting up the certificates and keys for ssl connection can be tricky see The test documentation for detailed examples.\n  sslfactory(String)\nThe provided value is a class name to use as the SSLSocketFactory when establishing a SSL connection. For more information see the section called Custom SSLSocketFactory defaults to LibPQFactory\n  sslfactoryarg (String) : (deprecated)\nThis value is an optional argument to the constructor of the sslfactory class provided above. For more information see the section called Custom SSLSocketFactory.\n  sslmode (String)\npossible values include disable , allow , prefer , require , verify-ca and verify-full . require , allow and prefer all default to a non validating SSL factory and do not check the validity of the certificate or the host name. verify-ca validates the certificate, but does not verify the hostname. verify-full will validate that the certificate is correct and verify the host connected to has the same hostname as the certificate. Default is prefer Setting these will necessitate storing the server certificate on the client machine see Configuring the client for details.\n  sslcert (String)\nProvide the full path for the certificate file. Defaults to /defaultdir/postgresql.crt, where defaultdir is ${user.home}/.postgresql/ in *nix systems and %appdata%/postgresql/ on windows.\nIt can be a PEM encoded X509v3 certificate\n   NOTE\nThis parameter is ignored when using PKCS-12 keys, since in that case the certificate is also retrieved from the same keyfile.\n  sslkey (String)\nProvide the full path for the key file. Defaults to /defaultdir/postgresql.pk8.   NOTE\nThe key file must be in PKCS-12 or in PKCS-8 DER format. A PEM key can be converted to DER format using the openssl command: openssl pkcs8 -topk8 -inform PEM -in postgresql.key -outform DER -out postgresql.pk8 -v1 PBE-MD5-DES\n PKCS-12 key files are only recognized if they have the “.p12” (42.2.9+) or the “.pfx” (42.2.16+) extension.\nIf your key has a password, provide it using the sslpassword connection parameter described below. Otherwise, you can add the flag -nocrypt to the above command to prevent the driver from requesting a password.\n NOTE\nThe use of -v1 PBE-MD5-DES might be inadequate in environments where high level of security is needed and the key is not protected by other means (e.g. access control of the OS), or the key file is transmitted in untrusted channels. We are depending on the cryptography providers provided by the java runtime. The solution documented here is known to work at the time of writing. If you have stricter security needs, please see here for a discussion of the problem and information on choosing a better cipher suite.\n   sslrootcert (String)\nFile name of the SSL root certificate. Defaults to defaultdir/root.crt. It can be a PEM encoded X509v3 certificate\n  sslhostnameverifier (String)\nClass name of hostname verifier. Defaults to using org.postgresql.ssl.PGjdbcHostnameVerifier\n  sslpasswordcallback (String)\nClass name of the SSL password provider. Defaults to org.postgresql.ssl.jdbc4.LibPQFactory.ConsoleCallbackHandler\n  sslpassword (String)\nIf provided will be used by ConsoleCallbackHandler\n  protocolVersion (int)\nThe driver supports the V3 frontend/backend protocols. The V3 protocol was introduced in 7.4 and the driver will by default try to connect using the V3 protocol.\n  loggerLevel (String)\nThis property is no longer used by the driver and will be ignored. All logging configuration is handled by java.util.logging.\n  loggerFile (String)\nThis property is no longer used by the driver and will be ignored. All logging configuration is handled by java.util.logging.\n  allowEncodingChanges (boolean)\nWhen using the V3 protocol the driver monitors changes in certain server configuration parameters that should not be touched by end users. The client_encoding setting is set by the driver and should not be altered. If the driver detects a change it will abort the connection. There is one legitimate exception to this behaviour though, using the COPY command on a file residing on the server’s filesystem. The only means of specifying the encoding of this file is by altering the client_encoding setting. The JDBC team considers this a failing of the COPY command and hopes to provide an alternate means of specifying the encoding in the future, but for now there is this URL parameter. Enable this only if you need to override the client encoding when doing a copy.\n  logUnclosedConnections (boolean)\nClients may leak Connection objects by failing to call its close() method. Eventually these objects will be garbage collected and the finalize() method will be called which will close the Connection if caller has neglected to do this himself. The usage of a finalizer is just a stopgap solution. To help developers detect and correct the source of these leaks the logUnclosedConnections URL parameter has been added. It captures a stacktrace at each Connection opening and if the finalize() method is reached without having been closed the stacktrace is printed to the log.\n  autosave (String)\nSpecifies what the driver should do if a query fails. In autosave=always mode, JDBC driver sets a savepoint before each query, and rolls back to that savepoint in case of failure. In autosave=never mode (default), no savepoint dance is made ever. In autosave=conservative mode, savepoint is set for each query, however the rollback is done only for rare cases like ‘cached statement cannot change return type’ or ‘statement XXX is not valid’ so JDBC driver rollsback and retries The default is never\n  cleanupSavepoints (boolean)\nDetermines if the SAVEPOINT created in autosave mode is released prior to the statement. This is done to avoid running out of shared buffers on the server in the case where 1000’s of queries are performed. The default is ‘false’\n  binaryTransfer (boolean)\nUse binary format for sending and receiving data if possible. The default is ’true’\n  binaryTransferEnable (String)\nA comma separated list of types to enable binary transfer. Either OID numbers or names.\n  binaryTransferDisable (String)\nA comma separated list of types to disable binary transfer. Either OID numbers or names. Overrides values in the driver default set and values set with binaryTransferEnable.\n  databaseMetadataCacheFields (int)\nSpecifies the maximum number of fields to be cached per connection. A value of 0 disables the cache. Defaults to 65536.\n  databaseMetadataCacheFieldsMiB (int)\nSpecifies the maximum size (in megabytes) of fields to be cached per connection. A value of 0 disables the cache. Defaults to 5.\n  prepareThreshold (int)\nDetermine the number of PreparedStatement executions required before switching over to use server side prepared statements. The default is five, meaning start using server side prepared statements on the fifth execution of the same PreparedStatement object. More information on server side prepared statements is available in the section called Server Prepared Statements.\n  preparedStatementCacheQueries (int)\nDetermine the number of queries that are cached in each connection. The default is 256, meaning if you use more than 256 different queries in prepareStatement() calls, the least recently used ones will be discarded. The cache allows application to benefit from Server Prepared Statements (see prepareThreshold ) even if the prepared statement is closed after each execution. The value of 0 disables the cache. N. B. Each connection has its own statement cache.\n  preparedStatementCacheSizeMiB (int)\nDetermine the maximum size (in mebibytes) of the prepared queries cache (see preparedStatementCacheQueries ). The default is 5, meaning if you happen to cache more than 5 MiB of queries the least recently used ones will be discarded. The main aim of this setting is to prevent OutOfMemoryError . The value of 0 disables the cache.\n  preferQueryMode (String)\nSpecifies which mode is used to execute queries to database: simple means (‘Q’ execute, no parse, no bind, text mode only), extended means always use bind/execute messages, extendedForPrepared means extended for prepared statements only, endedCacheEverything means use extended protocol and try cache every statement (including Statement.execute(String sql)) in a query cache. extended | extendedForPrepared | extendedCacheEverything | simple The default is extended\n  defaultRowFetchSize (int)\nDetermine the number of rows fetched in ResultSet by one fetch with trip to the database. Limiting the number of rows are fetch with each trip to the database allow avoids unnecessary memory consumption and as a consequence OutOfMemoryError . The default is zero, meaning that in ResultSet will be fetch all rows at once. Negative number is not available.\n  loginTimeout (int)\nSpecify how long to wait for establishment of a database connection. The timeout is specified in seconds.\n  connectTimeout (int)\nThe timeout value used for socket connect operations. If connecting to the server takes longer than this value, the connection is broken. The timeout is specified in seconds and a value of zero means that it is disabled.\n  socketTimeout (int)\nThe timeout value used for socket read operations. If reading from the server takes longer than this value, the connection is closed. This can be used as both a brute force global query timeout and a method of detecting network problems. The timeout is specified in seconds and a value of zero means that it is disabled.\n  cancelSignalTimeout (int)\nCancel command is sent out of band over its own connection, so cancel message can itself get stuck. This property controls “connect timeout” and “socket timeout” used for cancel commands. The timeout is specified in seconds. Default value is 10 seconds.\n  tcpKeepAlive (boolean)\nEnable or disable TCP keep-alive probe. The default is false .\n  tcpNoDelay (boolean)\nEnable or disable TCP nodelay. The default is true .\n  unknownLength (int)\nCertain postgresql types such as TEXT do not have a well defined length. When returning meta-data about these types through functions like ResultSetMetaData.getColumnDisplaySize and ResultSetMetaData.getPrecision we must provide a value and various client tools have different ideas about what they would like to see. This parameter specifies the length to return for types of unknown length.\n  stringtype (String)\nSpecify the type to use when binding PreparedStatement parameters set via setString() . If stringtype is set to VARCHAR (the default), such parameters will be sent to the server as varchar parameters. If stringtype is set to unspecified , parameters will be sent to the server as untyped values, and the server will attempt to infer an appropriate type. This is useful if you have an existing application that uses setString() to set parameters that are actually some other type, such as integers, and you are unable to change the application to use an appropriate method such as setInt() .\n  ApplicationName (String)\nSpecifies the name of the application that is using the connection. This allows a database administrator to see what applications are connected to the server and what resources they are using through views like pg_stat_activity.\n  kerberosServerName (String)\nThe Kerberos service name to use when authenticating with GSSAPI. This is equivalent to libpq’s PGKRBSRVNAME environment variable and defaults to “postgres”.\n  jaasApplicationName (String)\nSpecifies the name of the JAAS system or application login configuration.\n  jaasLogin (boolean)\nSpecifies whether to perform a JAAS login before authenticating with GSSAPI. If set to true (the default), the driver will attempt to obtain GSS credentials using the configured JAAS login module(s) (e.g. Krb5LoginModule ) before authenticating. To skip the JAAS login, for example if the native GSS implementation is being used to obtain credentials, set this to false .\n  gssEncMode (String)\nPostgreSQL 12 and later now allow GSSAPI encrypted connections. This parameter controls whether to enforce using GSSAPI encryption or not. The options are disable , allow , prefer and require\n disable is obvious and disables any attempt to connect using GSS encrypted mode allow will connect in plain text then if the server requests it will switch to encrypted mode prefer will attempt connect in encrypted mode and fall back to plain text if it fails to acquire an encrypted connection require attempts to connect in encrypted mode and will fail to connect if that is not possible. The default is allow .    gsslib (String)\nForce either SSPI (Windows transparent single-sign-on) or GSSAPI (Kerberos, via JSSE) to be used when the server requests Kerberos or SSPI authentication. Permissible values are auto (default, see below), sspi (force SSPI) or gssapi (force GSSAPI-JSSE). If this parameter is auto, SSPI is attempted if the server requests SSPI authentication, the JDBC client is running on Windows, and the Waffle libraries required for SSPI are on the CLASSPATH. Otherwise Kerberos/GSSAPI via JSSE is used.\n   Note\nThis behaviour does not exactly match that of libpq, which uses Windows’ SSPI libraries for Kerberos (GSSAPI) requests by default when on Windows.\n gssapi mode forces JSSE’s GSSAPI to be used even if SSPI is available, matching the pre-9.4 behaviour.\nOn non-Windows platforms or where SSPI is unavailable, forcing sspi mode will fail with a PSQLException. To use SSPI with PgJDBC you must ensure thatthe waffle-jna library and its dependencies are present on the CLASSPATH . pgJDBC does not bundle waffle-jna in the pgJDBC jar.\nSince: 9.4\n  sspiServiceClass (String)\nSpecifies the name of the Windows SSPI service class that forms the service class part of the SPN. The default, POSTGRES, is almost always correct. See: SSPI authentication (Pg docs) Service Principal Names (MSDN), DsMakeSpn (MSDN) Configuring SSPI (Pg wiki). This parameter is ignored on non-Windows platforms.\n  useSpnego (boolean)\nUse SPNEGO in SSPI authentication requests\n  sendBufferSize (int)\nSets SO_SNDBUF on the connection stream\n  receiveBufferSize (int)\nSets SO_RCVBUF on the connection stream\n  readOnly (boolean)\nPut the connection in read-only mode\n  readOnlyMode (String)\nOne of ‘ignore’, ’transaction’, or ‘always’. Controls the behavior when a connection is set to read only, When set to ‘ignore’ then the readOnly setting has no effect. When set to ’transaction’ and readOnly is set to ’true’ and autocommit is ‘false’ the driver will set the transaction to readonly by sending BEGIN READ ONLY . When set to ‘always’ and readOnly is set to ’true’ the session will be set to READ ONLY if autoCommit is ’true’. If autocommit is false the driver will set the transaction to read only by sending BEGIN READ ONLY . The default the value is ’transaction’\n  disableColumnSanitiser (boolean)\nSetting this to true disables column name sanitiser. The sanitiser folds columns in the resultset to lowercase. The default is to sanitise the columns (off).\n  assumeMinServerVersion (String)\nAssume that the server is at least the given version, thus enabling to some optimization at connection time instead of trying to be version blind.\n  currentSchema (String)\nSpecify the schema (or several schema separated by commas) to be set in the search-path. This schema will be used to resolve unqualified object names used in statements over this connection.\n  targetServerType (String)\nAllows opening connections to only servers with required state, the allowed values are any, primary, master, slave, secondary, preferSlave, preferSecondary and preferPrimary. The primary/secondary distinction is currently done by observing if the server allows writes. The value preferSecondary tries to connect to secondary if any are available, otherwise allows falls back to connecting also to primary. The value preferPrimary tries to connect to primary if it is available, otherwise allows falls back to connecting to secondaries available.\n N. B. the words master and slave are being deprecated. We will silently accept them, but primary and secondary are encouraged.    hostRecheckSeconds (int)\nControls how long in seconds the knowledge about a host state is cached in JVM wide global cache. The default value is 10 seconds.\n  loadBalanceHosts (boolean)\nIn default mode (disabled) hosts are connected in the given order. If enabled hosts are chosen randomly from the set of suitable candidates.\n  socketFactory (String)\nThe provided value is a class name to use as the SocketFactory when establishing a socket connection. This may be used to create unix sockets instead of normal sockets. The class name specified by socketFactory must extend javax.net.SocketFactory and be available to the driver’s classloader. This class must have a zero-argument constructor, a single-argument constructor taking a String argument, or a single-argument constructor taking a Properties argument. The Properties object will contain all the connection parameters. The String argument will have the value of the socketFactoryArg connection parameter.\n  socketFactoryArg (String) : (deprecated)\nThis value is an optional argument to the constructor of the socket factory class provided above.\n  reWriteBatchedInserts (boolean)\nThis will change batch inserts from insert into foo (col1, col2, col3) values (1, 2, 3) into insert into foo (col1, col2, col3) values (1, 2, 3), (4, 5, 6) this provides 2-3x performance improvement\n  replication (String)\nConnection parameter passed in the startup message. This parameter accepts two values; “true” and database . Passing true tells the backend to go into walsender mode, wherein a small set of replication commands can be issued instead of SQL statements. Only the simple query protocol can be used in walsender mode. Passing “database” as the value instructs walsender to connect to the database specified in the dbname parameter, which will allow the connection to be used for logical replication from that database. Parameter should be use together with assumeMinServerVersion with parameter \u003e= 9.4 (backend \u003e= 9.4)\n  escapeSyntaxCallMode (String)\nSpecifies how the driver transforms JDBC escape call syntax into underlying SQL, for invoking procedures or functions. In escapeSyntaxCallMode=select mode (the default), the driver always uses a SELECT statement (allowing function invocation only). In escapeSyntaxCallMode=callIfNoReturn mode, the driver uses a CALL statement (allowing procedure invocation) if there is no return parameter specified, otherwise the driver uses a SELECT statement. In escapeSyntaxCallMode=call mode, the driver always uses a CALL statement (allowing procedure invocation only). The default is select\n  maxResultBuffer (String)\nSpecifies size of result buffer in bytes, which can’t be exceeded during reading result set. Property can be specified in two styles:\n as size of bytes (i.e. 100, 150M, 300K, 400G, 1T); as percent of max heap memory (i.e. 10p, 15pct, 20percent); A limit during setting of property is 90% of max heap memory. All given values, which gonna be higher than limit, gonna lowered to the limit. By default, maxResultBuffer is not set (is null), what means that reading of results gonna be performed without limits.    adaptiveFetch (boolean)\nSpecifies if number of rows, fetched in ResultSet by one fetch with trip to the database, should be dynamic. Using dynamic number of rows, computed by adaptive fetch, allows to use most of the buffer declared in maxResultBuffer property. Number of rows would be calculated by dividing maxResultBuffer size into max row size observed so far, rounded down. First fetch will have number of rows declared in defaultRowFetchSize . Number of rows can be limited by adaptiveFetchMinimum and adaptiveFetchMaximum . Requires declaring of maxResultBuffer and defaultRowFetchSize to work. By default, adaptiveFetch is false.\n  adaptiveFetchMinimum (int)\nSpecifies the lowest number of rows which can be calculated by adaptiveFetch. Requires adaptiveFetch set to true to work. By default, minimum of rows calculated by adaptiveFetch is 0.\n  adaptiveFetchMaximum (int)\nSpecifies the highest number of rows which can be calculated by adaptiveFetch. Requires adaptiveFetch set to true to work. By default, maximum of rows calculated by adaptiveFetch is -1, which is understood as infinite.\n  logServerErrorDetail (boolean)\nWhether to include server error details in exceptions and log messages (for example inlined query parameters). Setting to false will only include minimal, not sensitive messages. By default this is set to true, server error details are propagated. This may include sensitive details such as query parameters.\n  quoteReturningIdentifiers (boolean)\nQuote returning columns. There are some ORM’s that quote everything, including returning columns If we quote them, then we end up sending ““colname”” to the backend instead of “colname” which will not be found.\n  authenticationPluginClassName (String)\nFully qualified class name of the class implementing the AuthenticationPlugin interface. If this is null, the password value in the connection properties will be used.\n  Unix sockets    By adding junixsocket you can obtain a socket factory that works with the driver. Code can be found at here and instructions at here\nDependencies for junixsocket are :\n\u003cdependency\u003e \u003cgroupId\u003ecom.kohlschutter.junixsocket\u003c/groupId\u003e \u003cartifactId\u003ejunixsocket-core\u003c/artifactId\u003e \u003cversion\u003e2.3.3\u003c/version\u003e \u003c/dependency\u003e Simply add ?socketFactory=org.newsclub.net.unix.AFUNIXSocketFactory$FactoryArg\u0026socketFactoryArg=[path-to-the-unix-socket] to the connection URL.\nFor many distros the default path is /var/run/postgresql/.s. PGSQL.5432\nConnection Fail-over    To support simple connection fail-over it is possible to define multiple endpoints (host and port pairs) in the connection url separated by commas. The driver will try once to connect to each of them in order until the connection succeeds. If none succeeds a normal connection exception is thrown.\nThe syntax for the connection url is: jdbc:postgresql://host1:port1,host2:port2/database\nThe simple connection fail-over is useful when running against a high availability postgres installation that has identical data on each node. For example streaming replication postgres or postgres-xc cluster.\nFor example an application can create two connection pools. One data source is for writes, another for reads. The write pool limits connections only to a primary node:jdbc:postgresql://node1,node2,node3/accounting?targetServerType=primary .\nAnd read pool balances connections between secondary nodes, but allows connections also to a primary if no secondaries are available: jdbc:postgresql://node1,node2,node3/accounting?targetServerType=preferSecondary\u0026loadBalanceHosts=true\nIf a secondary fails, all secondaries in the list will be tried first. In the case that there are no available secondaries the primary will be tried. If all the servers are marked as “can’t connect” in the cache then an attempt will be made to connect to all the hosts in the URL, in order.\n","description":"","title":"Initializing the Driver","uri":"/pgjdbc/documentation/use/"},{"categories":null,"content":"Configuring the PostgreSQL™ server for SSL is covered in the main documentation, so it will not be repeated here. There are also instructions in the source certdir Before trying to access your SSL enabled server from Java, make sure you can get to it via psql. You should see output like the following if you have established a SSL connection.\n$ ./bin/psql -h localhost -U postgres psql (9.6.2) SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off) Type \"help\" for help. postgres=# Custom SSLSocketFactory    PostgreSQL™ provides a way for developers to customize how a SSL connection is established. This may be used to provide a custom certificate source or other extensions by allowing the developer to create their own SSLContext instance. The connection URL parameters sslfactory allow the user to specify which custom class to use for creating the SSLSocketFactory . The class name specified by sslfactory must extend javax.net.ssl.SSLSocketFactory and be available to the driver’s classloader.\nThis class must have a zero argument constructor or a single argument constructor preferentially taking a Properties argument. There is a simple org.postgresql.ssl.DefaultJavaSSLFactory provided which uses the default java SSLFactory.\nInformation on how to actually implement such a class is beyond the scope of this documentation. Places to look for help are the JSSE Reference Guide and the source to the NonValidatingFactory provided by the JDBC driver.\nConfiguring the Client    There are a number of connection parameters for configuring the client for SSL. See SSL Connection parameters\nThe simplest being ssl=true , passing this into the driver will cause the driver to validate both the SSL certificate and verify the hostname (same as verify-full ).\n Note\nThis is different than libpq which defaults to a non-validating SSL connection.\n In this mode, when establishing a SSL connection the JDBC driver will validate the server’s identity preventing “man in the middle” attacks. It does this by checking that the server certificate is signed by a trusted authority, and that the host you are connecting to is the same as the hostname in the certificate.\nIf you require encryption and want the connection to fail if it can’t be encrypted then set sslmode=require this ensures that the server is configured to accept SSL connections for this Host/IP address and that the server recognizes the client certificate. In other words if the server does not accept SSL connections or the client certificate is not recognized the connection will fail.\n Note\nIn this mode we will accept all server certificates.\n If sslmode=verify-ca , the server is verified by checking the certificate chain up to the root certificate stored on the client.\nIf sslmode=verify-full , the server host name will be verified to make sure it matches the name stored in the server certificate.\nThe SSL connection will fail if the server certificate cannot be verified. verify-full is recommended in most security-sensitive environments.\nThe default SSL Socket factory is the LibPQFactory. In the case where the certificate validation is failing you can try sslcert= and LibPQFactory will not send the client certificate. If the server is not configured to authenticate using the certificate it should connect.\nThe location of the client certificate, the PKCS-8 client key and root certificate can be overridden with the sslcert , sslkey , and sslrootcert settings respectively. These default to /defaultdir/postgresql.crt, /defaultdir/postgresql.pk8 , and /defaultdir/root.crt respectively where defaultdir is ${user.home}/.postgresql/ in *nix systems and %appdata%/postgresql/ on windows\nAs of version 42.2.9 PKCS-12 is also supported. In this archive format the client key and the client certificate are in one file, which needs to be set with the sslkey parameter. For the PKCS-12 format to be recognized, the file extension must be “.p12” (supported since 42.2.9) or “.pfx” (since 42.2.16). (In this case the sslcert parameter is ignored.)\nFiner control of the SSL connection can be achieved using the sslmode connection parameter. This parameter is the same as the libpq sslmode parameter and currently implements the following\n   sslmode Eavesdropping Protection MITM Protection      disable No No I don’t care about security and don’t want to pay the overhead for encryption   allow Maybe No I don’t care about security but will pay the overhead for encryption if the server insists on it   prefer Maybe No I don’t care about encryption but will pay the overhead of encryption if the server supports it   require Yes No I want my data to be encrypted, and I accept the overhead. I trust that the network will make sure I always connect to the server I want.   verify-ca Yes Depends on CA policy I want my data encrypted, and I accept the overhead. I want to be sure that I connect to a server that I trust.   verify-full Yes Yes I want my data encrypted, and I accept the overhead. I want to be sure that I connect to a server I trust, and that it’s the one I specify.     NOTE\nIf you are using Java’s default mechanism (not LibPQFactory) to create the SSL connection you will need to make the server certificate available to Java, the first step is to convert it to a form Java understands.\n openssl x509 -in server.crt -out server.crt.der -outform der\nFrom here the easiest thing to do is import this certificate into Java’s system truststore.\nkeytool -keystore $JAVA_HOME/lib/security/cacerts -alias postgresql -import -file server.crt.der\nThe default password for the cacerts keystore is changeit . Setting the alias to postgresql is not required. You may apply any name you wish.\nIf you do not have access to the system cacerts truststore you can create your own truststore.\nkeytool -keystore mystore -alias postgresql -import -file server.crt.der\nWhen starting your Java application you must specify this keystore and password to use.\njava -Djavax.net.ssl.trustStore=mystore -Djavax.net.ssl.trustStorePassword=mypassword com.mycompany.MyApp\nIn the event of problems extra debugging information is available by adding -Djavax.net.debug=ssl to your command line.\nUsing SSL without Certificate Validation    In some situations it may not be possible to configure your Java environment to make the server certificate available, for example in an applet. For a large scale deployment it would be best to get a certificate signed by recognized certificate authority, but that is not always an option. The JDBC driver provides an option to establish a SSL connection without doing any validation, but please understand the risk involved before enabling this option.\nA non-validating connection is established via a custom SSLSocketFactory class that is provided with the driver. Setting the connection URL parameter sslfactory=org.postgresql.ssl.NonValidatingFactory will turn off all SSL validation.\n","description":"","title":"Using SSL","uri":"/pgjdbc/documentation/ssl/"},{"categories":null,"content":"Any time you want to issue SQL statements to the database, you require a Statement or PreparedStatement instance. Once you have a Statement or PreparedStatement , you can use issue a query. This will return a ResultSet instance, which contains the entire result (see the section called Getting results based on a cursor here for how to alter this behaviour). Example 5.1, “Processing a Simple Query in JDBC” illustrates this process.\nExample 5.1. Processing a Simple Query in JDBC    This example will issue a simple query and print out the first column of each row using a Statement .\nStatement st = conn.createStatement(); ResultSet rs = st.executeQuery(\"SELECT * FROM mytable WHERE columnfoo = 500\"); while (rs.next()) { System.out.print(\"Column 1 returned \"); System.out.println(rs.getString(1)); } rs.close(); st.close(); This example issues the same query as before but uses a PreparedStatement and a bind value in the query.\nint foovalue = 500; PreparedStatement st = conn.prepareStatement(\"SELECT * FROM mytable WHERE columnfoo = ?\"); st.setInt(1, foovalue); ResultSet rs = st.executeQuery(); while (rs.next()) { System.out.print(\"Column 1 returned \"); System.out.println(rs.getString(1)); } rs.close(); st.close(); Getting results based on a cursor    By default the driver collects all the results for the query at once. This can be inconvenient for large data sets so the JDBC driver provides a means of basing a ResultSet on a database cursor and only fetching a small number of rows.\nA small number of rows are cached on the client side of the connection and when exhausted the next block of rows is retrieved by repositioning the cursor.\n NOTE\nCursor based ResultSets cannot be used in all situations. There a number of restrictions which will make the driver silently fall back to fetching the whole ResultSet at once.\n  The connection to the server must be using the V3 protocol. This is the default for (and is only supported by) server versions 7.4 and later.\n  The Connection must not be in autocommit mode. The backend closes cursors at the end of transactions, so in autocommit mode the backend will have closed the cursor before anything can be fetched from it.\n  The Statement must be created with a ResultSet type of ResultSet.TYPE_FORWARD_ONLY. This is the default, so no code will need to be rewritten to take advantage of this, but it also means that you cannot scroll backwards or otherwise jump around in the ResultSet.\n  The query given must be a single statement, not multiple statements strung together with semicolons.\n   Example 5.2. Setting fetch size to turn cursors on and off.    Changing code to cursor mode is as simple as setting the fetch size of the Statement to the appropriate size. Setting the fetch size back to 0 will cause all rows to be cached (the default behaviour).\n// make sure autocommit is off conn.setAutoCommit(false); Statement st = conn.createStatement(); // Turn use of the cursor on. st.setFetchSize(50); ResultSet rs = st.executeQuery(\"SELECT * FROM mytable\"); while (rs.next()) { System.out.print(\"a row was returned.\"); } rs.close(); // Turn the cursor off. st.setFetchSize(0); rs = st.executeQuery(\"SELECT * FROM mytable\"); while (rs.next()) { System.out.print(\"many rows were returned.\"); } rs.close(); // Close the statement. st.close(); Using the Statement or PreparedStatement Interface    The following must be considered when using the Statement or PreparedStatement interface:\n  You can use a single Statement instance as many times as you want. You could create one as soon as you open the connection and use it for the connection’s lifetime. But you have to remember that only one ResultSet can exist per Statement or PreparedStatement at a given time.\n  If you need to perform a query while processing a ResultSet, you can simply create and use another Statement .\n  If you are using threads, and several are using the database, you must use a separate Statement for each thread. Refer to Using the Driver in a Multithreaded or a Servlet Environment if you are thinking of using threads, as it covers some important points.\n  When you are done using the Statement or PreparedStatement you should close it.\n  In JDBC, the question mark (?) is the placeholder for the positional parameters of a PreparedStatement. There are, however, a number of PostgreSQL operators that contain a question mark. To keep such question marks in a SQL statement from being interpreted as positional parameters, use two question marks ( ?? ) as escape sequence. You can also use this escape sequence in a Statement , but that is not required. Specifically only in a Statement a single ( ? ) can be used as an operator.\n  Using the ResultSet Interface    The following must be considered when using the ResultSet interface:\n  Before reading any values, you must call next(). This returns true if there is a result, but more importantly, it prepares the row for processing.\n  You must close a ResultSet by calling close() once you have finished using it.\n  Once you make another query with the Statement used to create a ResultSet, the currently open ResultSet instance is closed automatically.\n  When PreparedStatement API is used, ResultSet switches to binary mode after five query executions (this default is set by the prepareThreshold connection property, see Server Prepared Statements. This may cause unexpected behavior when some methods are called. For example, results on method calls such as getString() on non-string data types, while logically equivalent, may be formatted differently after execution exceeds the set prepareThreshold when conversion to object method switches to one matching the return mode.\n  Performing Updates    To change data (perform an INSERT , UPDATE , or DELETE ) you use the executeUpdate() method. This method is similar to the method executeQuery()\nused to issue a SELECT statement, but it doesn’t return a ResultSet instead it returns the number of rows affected by the INSERT , UPDATE , or DELETE statement. Example 5.3, “Deleting Rows in JDBC” illustrates the usage.\nExample 5.3. Deleting Rows in JDBC    This example will issue a simple DELETE statement and print out the number of rows deleted.\nint foovalue = 500; PreparedStatement st = conn.prepareStatement(\"DELETE FROM mytable WHERE columnfoo = ?\"); st.setInt(1, foovalue); int rowsDeleted = st.executeUpdate(); System.out.println(rowsDeleted + \" rows deleted\"); st.close(); Creating and Modifying Database Objects    To create, modify or drop a database object like a table or view you use the execute() method. This method is similar to the method executeQuery() , but it doesn’t return a result.Example 5.4, “Dropping a Table in JDBC illustrates the usage.\nExample 5.4. Dropping a Table in JDBC    This example will drop a table.\nStatement st = conn.createStatement(); st.execute(\"DROP TABLE mytable\"); st.close(); Using Java 8 Date and Time classes    The PostgreSQL™ JDBC driver implements native support for the Java 8 Date and Time API(JSR-310) using JDBC 4.2.\nTable 5.1. Supported Java 8 Date and Time classes       PostgreSQL™ Java SE 8     DATE LocalDate   TIME [ WITHOUT TIME ZONE ] LocalTime   TIMESTAMP [ WITHOUT TIME ZONE ] LocalDateTime   TIMESTAMP WITH TIME ZONE OffsetDateTime    This is closely aligned with tables B-4 and B-5 of the JDBC 4.2 specification.\n Note\nZonedDateTime , Instant and OffsetTime / TIME WITH TIME ZONE are not supported. Also note that all OffsetDateTime instances will have be in UTC (have offset 0). This is because the backend stores them as UTC.\n Example 5.2. Reading Java 8 Date and Time values using JDBC\nStatement st = conn.createStatement(); ResultSet rs = st.executeQuery(\"SELECT * FROM mytable WHERE columnfoo = 500\"); while (rs.next()) { System.out.print(\"Column 1 returned \"); LocalDate localDate = rs.getObject(1, LocalDate.class); System.out.println(localDate); } rs.close(); st.close(); For other data types simply pass other classes to #getObject .\n Note\nThe Java data types needs to match the SQL data types in table 7.1.\n Example 5.3. Writing Java 8 Date and Time values using JDBC    LocalDate localDate = LocalDate.now(); PreparedStatement st = conn.prepareStatement(\"INSERT INTO mytable (columnfoo) VALUES (?)\"); st.setObject(1, localDate); st.executeUpdate(); st.close(); ","description":"","title":"Issuing a Query and Processing the Result","uri":"/pgjdbc/documentation/query/"},{"categories":null,"content":"PostgreSQL™ supports two types of stored objects, functions that can return a result value and - starting from v11 - procedures that can perform transaction control. Both types of stored objects are invoked using CallableStatement and the standard JDBC escape call syntax {call storedobject(?)} . The escapeSyntaxCallMode connection property controls how the driver transforms the call syntax to invoke functions or procedures.\nThe default mode, select , supports backwards compatibility for existing applications and supports function invocation only. This is required to invoke a void returning function.\nFor new applications, use escapeSyntaxCallMode=callIfNoReturn to map CallableStatements with return values to stored functions and CallableStatements without return values to stored procedures.\nExample 6.1. Calling a built in stored function    This example shows how to call a PostgreSQL™ built in function, upper, which simply converts the supplied string argument to uppercase.\nCallableStatement upperFunc = conn.prepareCall(\"{? = call upper( ? ) }\"); upperFunc.registerOutParameter(1, Types.VARCHAR); upperFunc.setString(2, \"lowercase to uppercase\"); upperFunc.execute(); String upperCased = upperFunc.getString(1); upperFunc.close(); Obtaining a ResultSet from a stored function    PostgreSQL’s™ stored functions can return results in two different ways. The function may return either a refcursor value or a SETOF some datatype. Depending on which of these return methods are used determines how the function should be called.\nFrom a Function Returning SETOF type    Functions that return data as a set should not be called via the CallableStatement interface, but instead should use the normal Statement or PreparedStatement interfaces.\nExample 6.2. Getting SETOF type values from a function    Statement stmt = conn.createStatement(); stmt.execute(\"CREATE OR REPLACE FUNCTION setoffunc() RETURNS SETOF int AS \" + \"' SELECT 1 UNION SELECT 2;' LANGUAGE sql\"); ResultSet rs = stmt.executeQuery(\"SELECT * FROM setoffunc()\"); while (rs.next()) { // do something } rs.close(); stmt.close(); From a Function Returning a refcursor    When calling a function that returns a refcursor you must cast the return type of getObject to a ResultSet`\n NOTE\nOne notable limitation of the current support for a ResultSet created from a refcursor is that even though it is a cursor backed ResultSet , all data will be retrieved and cached on the client. The Statement fetch size parameter described in the section called Getting results based on a cursor is ignored. This limitation is a deficiency of the JDBC driver, not the server, and it is technically possible to remove it, we just haven’t found the time.\n Example 6.3. Getting refcursor Value From a Function    // Setup function to call. Statement stmt = conn.createStatement(); stmt.execute(\"CREATE OR REPLACE FUNCTION refcursorfunc() RETURNS refcursor AS '\" + \" DECLARE \" + \" mycurs refcursor; \" + \" BEGIN \" + \" OPEN mycurs FOR SELECT 1 UNION SELECT 2; \" + \" RETURN mycurs; \" + \" END;' language plpgsql\"); stmt.close(); // We must be inside a transaction for cursors to work. conn.setAutoCommit(false); // Function call. CallableStatement func = conn.prepareCall(\"{? = call refcursorfunc() }\"); func.registerOutParameter(1, Types.OTHER); func.execute(); ResultSet results = (ResultSet) func.getObject(1); while (results.next()) { // do something with the results. } results.close(); func.close(); It is also possible to treat the refcursor return value as a cursor name directly. To do this, use the getString of ResultSet . With the underlying cursor name, you are free to directly use cursor commands on it, such as FETCH and MOVE .\nExample 6.4. Treating refcursor as a cursor name    conn.setAutoCommit(false); CallableStatement func = conn.prepareCall(\"{? = call refcursorfunc() }\"); func.registerOutParameter(1, Types.OTHER); func.execute(); String cursorName = func.getString(1); func.close(); Example 6.5. Calling a stored procedure    This example shows how to call a PostgreSQL™ procedure that uses transaction control.\n// set up a connection String url = \"jdbc:postgresql://localhost/test\"; Properties props = new Properties(); ...other properties... // Ensure EscapeSyntaxCallmode property set to support procedures if no return value  props.setProperty(\"escapeSyntaxCallMode\", \"callIfNoReturn\"); Connection con = DriverManager.getConnection(url, props); // Setup procedure to call. Statement stmt = con.createStatement(); stmt.execute(\"CREATE TEMP TABLE temp_val ( some_val bigint )\"); stmt.execute(\"CREATE OR REPLACE PROCEDURE commitproc(a INOUT bigint) AS '\" + \" BEGIN \" + \" INSERT INTO temp_val values(a); \" + \" COMMIT; \" + \" END;' LANGUAGE plpgsql\"); stmt.close(); // As of v11, we must be outside a transaction for procedures with transactions to work. con.setAutoCommit(true); // Procedure call with transaction CallableStatement proc = con.prepareCall(\"{call commitproc( ? )}\"); proc.setInt(1, 100); proc.execute(); proc.close(); ","description":"","title":"Calling Stored Functions and Procedures","uri":"/pgjdbc/documentation/callproc/"},{"categories":null,"content":"PostgreSQL™ provides two distinct ways to store binary data. Binary data can be stored in a table using the data type BYTEA or by using the Large Object feature which stores the binary data in a separate table in a special format and refers to that table by storing a value of type OID in your table.\nIn order to determine which method is appropriate you need to understand the limitations of each method. The BYTEA data type is not well suited for storing very large amounts of binary data. While a column of type BYTEA can hold up to 1 GB of binary data, it would require a huge amount of memory to process such a large value. The Large Object method for storing binary data is better suited to storing very large values, but it has its own limitations. Specifically deleting a row that contains a Large Object reference does not delete the Large Object. Deleting the Large Object is a separate operation that needs to be performed. Large Objects also have some security issues since anyone connected to the database can view and/or modify any Large Object, even if they don’t have permissions to view/update the row containing the Large Object reference.\nVersion 7.2 was the first release of the JDBC driver that supports the BYTEA data type. The introduction of this functionality in 7.2 has introduced a change in behavior as compared to previous releases. Since 7.2, the methods getBytes() , setBytes() , getBinaryStream() , and setBinaryStream() operate on the BYTEA data type. In 7.1 and earlier, these methods operated on the OID data type associated with Large Objects. It is possible to revert the driver back to the old 7.1 behavior by setting the property compatible on the Connection object to the value 7.1 . More details on connection properties are available in the section called Connection Parameters.\nTo use the BYTEA data type you should simply use the getBytes() , setBytes() , getBinaryStream() , or setBinaryStream() methods.\nTo use the Large Object functionality you can use either the LargeObject class provided by the PostgreSQL™ JDBC driver, or by using the getBLOB() and setBLOB() methods.\n IMPORTANT\nYou must access Large Objects within an SQL transaction block. You can start a transaction block by calling setAutoCommit(false) .\n Example 7.1, “Processing Binary Data in JDBC” contains some examples on how to process binary data using the PostgreSQL™ JDBC driver.\nExample 7.1. Processing Binary Data in JDBC    For example, suppose you have a table containing the file names of images and you also want to store the image in a BYTEA column:\nCREATETABLEimages(imgnametext,imgbytea);To insert an image, you would use:\nFile file = new File(\"myimage.gif\"); FileInputStream fis = new FileInputStream(file); PreparedStatement ps = conn.prepareStatement(\"INSERT INTO images VALUES (?, ?)\"); ps.setString(1, file.getName()); ps.setBinaryStream(2, fis, (int) file.length()); ps.executeUpdate(); ps.close(); fis.close(); Here, setBinaryStream() transfers a set number of bytes from a stream into the column of type BYTEA. This also could have been done using the setBytes() method if the contents of the image was already in a byte[] .\n NOTE\nThe length parameter to setBinaryStream must be correct. There is no way to indicate that the stream is of unknown length. If you are in this situation, you must read the stream yourself into temporary storage and determine the length. Now with the correct length you may send the data from temporary storage on to the driver.\n Retrieving an image is even easier. (We use PreparedStatement here, but the Statement class can equally be used.\nPreparedStatement ps = conn.prepareStatement(\"SELECT img FROM images WHERE imgname = ?\"); ps.setString(1, \"myimage.gif\"); ResultSet rs = ps.executeQuery(); while (rs.next()) { byte[] imgBytes = rs.getBytes(1); // use the data in some way here } rs.close(); ps.close(); Here the binary data was retrieved as an byte[] . You could have used a InputStream object instead.\nAlternatively you could be storing a very large file and want to use the LargeObject API to store the file:\nCREATETABLEimageslo(imgnametext,imgoidoid);To insert an image, you would use:\n// All LargeObject API calls must be within a transaction block conn.setAutoCommit(false); // Get the Large Object Manager to perform operations with LargeObjectManager lobj = conn.unwrap(org.postgresql.PGConnection.class).getLargeObjectAPI(); // Create a new large object long oid = lobj.createLO(LargeObjectManager.READ | LargeObjectManager.WRITE); // Open the large object for writing LargeObject obj = lobj.open(oid, LargeObjectManager.WRITE); // Now open the file File file = new File(\"myimage.gif\"); FileInputStream fis = new FileInputStream(file); // Copy the data from the file to the large object byte buf[] = new byte[2048]; int s, tl = 0; while ((s = fis.read(buf, 0, 2048)) \u003e 0) { obj.write(buf, 0, s); tl += s; } // Close the large object obj.close(); // Now insert the row into imageslo PreparedStatement ps = conn.prepareStatement(\"INSERT INTO imageslo VALUES (?, ?)\"); ps.setString(1, file.getName()); ps.setLong(2, oid); ps.executeUpdate(); ps.close(); fis.close(); // Finally, commit the transaction. conn.commit(); Retrieving the image from the Large Object:\n// All LargeObject API calls must be within a transaction block conn.setAutoCommit(false); // Get the Large Object Manager to perform operations with LargeObjectManager lobj = conn.unwrap(org.postgresql.PGConnection.class).getLargeObjectAPI(); PreparedStatement ps = conn.prepareStatement(\"SELECT imgoid FROM imageslo WHERE imgname = ?\"); ps.setString(1, \"myimage.gif\"); ResultSet rs = ps.executeQuery(); while (rs.next()) { // Open the large object for reading  long oid = rs.getLong(1); LargeObject obj = lobj.open(oid, LargeObjectManager.READ); // Read the data  byte buf[] = new byte[obj.size()]; obj.read(buf, 0, obj.size()); // Do something with the data read here  // Close the object  obj.close(); } rs.close(); ps.close(); // Finally, commit the transaction. conn.commit(); ","description":"","title":"Storing Binary Data","uri":"/pgjdbc/documentation/binary-data/"},{"categories":null,"content":"The JDBC specification (like the ODBC specification) acknowledges the fact that some vendor specific SQL may be required for certain RDBMS features. To aid developers in writing portable JDBC applications across multiple database products, a special escape syntax is used to specify the generic commands the developer wants to be run. The JDBC driver translates these escape sequences into native syntax for its specific database. For more information consult the Java DB Technical Documentation.\nThe parsing of the sql statements for these escapes can be disabled using Statement.setEscapeProcessing(false) .\nConnection.nativeSQL(String sql) provides another way to have escapes processed. It translates the given SQL to a SQL suitable for the PostgreSQL™ backend.\nExample 8.1. Using JDBC escapes    To use the JDBC escapes, you simply write your SQL replacing date/time literal values, outer join and functions by the JDBC escape syntax. For example :\nResultSet rs = st.executeQuery(\"SELECT {fn week({d '2005-01-24'})}\"); is the portable version for\nResultSet rs = st.executeQuery(\"SELECT extract(week from DATE '2005-01-24')\"); Escape for like escape character    You can specify which escape character to use in strings comparison (with LIKE ) to protect wildcards characters (’%’ and ‘_’) by adding the following escape : {escape 'escape-character'} . The driver supports this only at the end of the comparison expression.\nFor example, you can compare string values using ‘|’ as escape character to protect ‘_’ :\nrs = stmt.executeQuery(\"select str2 from comparisontest where str1 like '|_abcd' {escape '|'} \"); Escape for outer joins    You can specify outer joins using the following syntax: {oj table (LEFT|RIGHT|FULL) OUTER JOIN (table | outer-join) ON search-condition}\nFor example :\nResultSet rs = stmt.executeQuery(\"select * from {oj a left outer join b on (a.i=b.i)} \"); Date-time escapes    The JDBC specification defines escapes for specifying date, time and timestamp values which are supported by the driver.\n date : {d 'yyyy-mm-dd'} which is translated to DATE 'yyyy-mm-dd' time : {t 'hh:mm:ss'} which is translated to TIME 'hh:mm:ss' timestamp : {ts 'yyyy-mm-dd hh:mm:ss.f...'} which is translated to TIMESTAMP 'yyyy-mm-dd hh:mm:ss.f' The fractional seconds (.f…) portion of the TIMESTAMP can be omitted.  Escaped scalar functions    The JDBC specification defines functions with an escape call syntax : {fn function_name(arguments)} . The following tables show which functions are supported by the PostgreSQL™ driver. The driver supports the nesting and the mixing of escaped functions and escaped values. The appendix C of the JDBC specification describes the functions.\nSome functions in the following tables are translated but reported as not supported because they are duplicating or changing their order of the arguments. While this is harmless for literal values or columns, it will cause problems when using prepared statements. For example \" {fn right(?,?)} \" will be translated to \" substring(? from (length(?)+1-?)) “. As you can see the translated SQL requires more parameters than before the translation but the driver will not automatically handle this.\nTable 8.1. Supported escaped numeric functions       function reported as supported translation comments     abs(arg1) yes abs(arg1)    acos(arg1) yes acos(arg1)    asin(arg1) yes asin(arg1)    atan(arg1) yes atan(arg1)    atan2(arg1, arg2) yes atan2(arg1, arg2)    ceiling(arg1) yes ceil(arg1)    cos(arg1) yes cos(arg1)    cot(arg1) yes cot(arg1)    degrees(arg1) yes degrees(arg1)    exp(arg1) yes exp(arg1)    floor(arg1) yes floor(arg1)    log(arg1) yes ln(arg1)    log10(arg1) yes log(arg1)    mod(arg1, arg2) yes mod(arg1, arg2)    pi(arg1) yes pi(arg1)    power(arg1, arg2) yes pow(arg1, arg2)    radians(arg1) yes radians(arg1)    rand() yes random()    rand(arg1) yes setseed(arg1)*0+random() The seed is initialized with the given argument and a new random value is returned.   round(arg1, arg2) yes round(arg1, arg2)    sign(arg1) yes sign(arg1)    sin(arg1) yes sin(arg1)    sqrt(arg1) yes sqrt(arg1)    tan(arg1) yes tan(arg1)    truncate(arg1, arg2) yes trunc(arg1, arg2)     Table 8.2. Supported escaped string functions       function reported as supported translation comments     ascii(arg1) yes ascii(arg1)    char(arg1) yes chr(arg1)    concat(arg1, arg2…) yes (arg1    only require the two arguments version, but supporting more arguments      was so easy…      insert(arg1, arg2, arg3, arg4) no overlay(arg1 placing arg4 from arg2 for arg3) This function is not supported since it changes the order of the arguments which can be a problem (for prepared   statements by example).      lcase(arg1) yes lower(arg1)    left(arg1, arg2) yes substring(arg1 for arg2)    length(arg1) yes length(trim(trailing from arg1))    locate(arg1, arg2) no position(arg1 in arg2)    locate(arg1, arg2, arg3) no (arg2*sign(position(arg1 in substring(arg2 from arg3)+position(arg1 in substring(arg2 from arg3)) Not supported since the three arguments version duplicate and change the order of the arguments.   ltrim(arg1) yes trim(leading from arg1)    repeat(arg1, arg2) yes repeat(arg1, arg2)    replace(arg1, arg2, arg3) yes replace(arg1, arg2, arg3) Only reported as supported by 7.3 and above servers.   right(arg1, arg2) no substring(arg1 from (length(arg1)+1-arg2)) Not supported since arg2 is duplicated.   rtrim(arg1) yes trim(trailing from arg1)    space(arg1) yes repeat(’ ‘, arg1)    substring(arg1, arg2) yes substr(arg1, arg2)    substring(arg1, arg2, arg3) yes substr(arg1, arg2, arg3)    ucase(arg1) yes upper(arg1)    soundex(arg1) no soundex(arg1) Not supported since it requires the fuzzystrmatch contrib module.   difference(arg1, arg2) no difference(arg1, arg2) Not supported since it requires the fuzzystrmatch contrib module.    Table 8.3. Supported escaped date/time functions       function reported as supported translation comments     curdate() yes current_date    curtime() yes current_time    dayname(arg1) yes to_char(arg1, ‘Day’)    dayofmonth(arg1) yes extract(day from arg1)    dayofweek(arg1) yes extract(dow from arg1)+1 We must add 1 to be in the expected 1-7 range.   dayofyear(arg1) yes extract(doy from arg1)    hour(arg1) yes extract(hour from arg1)    minute(arg1) yes extract(minute from arg1)    month(arg1) yes extract(month from arg1)    monthname(arg1) yes to_char(arg1, ‘Month’)    now() yes now()    quarter(arg1) yes extract(quarter from arg1)    second(arg1) yes extract(second from arg1)    week(arg1) yes extract(week from arg1)    year(arg1) yes extract(year from arg1)    timestampadd(argIntervalType, argCount, argTimeStamp) yes (’(interval according to argIntervalType and    argCount)’+argTimeStamp) an argIntervalType value of SQL_TSI_FRAC_SECOND     is not implemented since backend does not support it      timestampdiff(argIntervalType, argTimeStamp1, argTimeStamp2) not extract((interval according to argIntervalType) from argTimeStamp2-argTimeStamp1 ) only an argIntervalType value of SQL_TSI_FRAC_SECOND, SQL_TSI_FRAC_MINUTE, SQL_TSI_FRAC_HOUR or SQL_TSI_FRAC_DAY is supported    Table 8.4. Supported escaped misc functions       function reported as supported translation comments     database() yes current_database() Only reported as supported by 7.3 and above servers.   ifnull(arg1, arg2) yes coalesce(arg1, arg2)    user() yes user     ","description":"","title":"JDBC escapes","uri":"/pgjdbc/documentation/escapes/"},{"categories":null,"content":"PostgreSQL™ is an extensible database system. You can add your own functions to the server, which can then be called from queries, or even add your own data types. As these are facilities unique to PostgreSQL™, we support them from Java, with a set of extension APIs. Some features within the core of the standard driver actually use these extensions to implement Large Objects, etc.\nAccessing the Extensions    To access some of the extensions, you need to use some extra methods in the org.postgresql.PGConnection class. In this case, you would need to cast the return value of Driver.getConnection() . For example:\nConnection db = Driver.getConnection(url, username, password); // ... // later on Fastpath fp = db.unwrap(org.postgresql.PGConnection.class).getFastpathAPI(); Geometric Data Types    PostgreSQL™ has a set of data types that can store geometric features into a table. These include single points, lines, and polygons. We support these types in Java with the org.postgresql.geometric package. Please consult the Javadoc mentioned in Further Reading for details of available classes and features.\nExample 9.1. Using the CIRCLE datatype JDBC    import java.sql.*; import org.postgresql.geometric.PGpoint; import org.postgresql.geometric.PGcircle; public class GeometricTest { public static void main(String args[]) throws Exception { String url = \"jdbc:postgresql://localhost:5432/test\"; try (Connection conn = DriverManager.getConnection(url, \"test\", \"\")) { try (Statement stmt = conn.createStatement()) { stmt.execute(\"CREATE TEMP TABLE geomtest(mycirc circle)\"); } insertCircle(conn); retrieveCircle(conn); } } private static void insertCircle(Connection conn) throws SQLException { PGpoint center = new PGpoint(1, 2.5); double radius = 4; PGcircle circle = new PGcircle(center, radius); try (PreparedStatement ps = conn.prepareStatement(\"INSERT INTO geomtest(mycirc) VALUES (?)\")) { ps.setObject(1, circle); ps.executeUpdate(); } } private static void retrieveCircle(Connection conn) throws SQLException { try (Statement stmt = conn.createStatement()) { try (ResultSet rs = stmt.executeQuery(\"SELECT mycirc, area(mycirc) FROM geomtest\")) { while (rs.next()) { PGcircle circle = (PGcircle) rs.getObject(1); double area = rs.getDouble(2); System.out.println(\"Center (X, Y) = (\" + circle.center.x + \", \" + circle.center.y + \")\"); System.out.println(\"Radius = \" + circle.radius); System.out.println(\"Area = \" + area); } } } } } Large Objects    Large objects are supported in the standard JDBC specification. However, that interface is limited, and the API provided by PostgreSQL™ allows for random access to the objects contents, as if it was a local file.\nThe org.postgresql.largeobject package provides to Java the libpq C interface’s large object API. It consists of two classes, LargeObjectManager , which deals with creating, opening and deleting large objects, and LargeObject which deals with an individual object. For an example usage of this API, please see Processing Binary Data in JDBC.\nListen / Notify    Listen and Notify provide a simple form of signal or interprocess communication mechanism for a collection of processes accessing the same PostgreSQL™ database. For more information on notifications consult the main server documentation. This section only deals with the JDBC specific aspects of notifications.\nStandard LISTEN , NOTIFY , and UNLISTEN commands are issued via the standard Statement interface. To retrieve and process retrieved notifications the Connection must be cast to the PostgreSQL™ specific extension interface PGConnection . From there the getNotifications() method can be used to retrieve any outstanding notifications.\n NOTE\nA key limitation of the JDBC driver is that it cannot receive asynchronous notifications and must poll the backend to check if any notifications were issued. A timeout can be given to the poll function, but then the execution of statements from other threads will block.\n Example 9.2. Receiving Notifications    import java.sql.*; public class NotificationTest { public static void main(String args[]) throws Exception { Class.forName(\"org.postgresql.Driver\"); String url = \"jdbc:postgresql://localhost:5432/test\"; // Create two distinct connections, one for the notifier  // and another for the listener to show the communication  // works across connections although this example would  // work fine with just one connection.  Connection lConn = DriverManager.getConnection(url, \"test\", \"\"); Connection nConn = DriverManager.getConnection(url, \"test\", \"\"); // Create two threads, one to issue notifications and  // the other to receive them.  Listener listener = new Listener(lConn); Notifier notifier = new Notifier(nConn); listener.start(); notifier.start(); } } class Listener extends Thread { private Connection conn; private org.postgresql.PGConnection pgconn; Listener(Connection conn) throws SQLException { this.conn = conn; this.pgconn = conn.unwrap(org.postgresql.PGConnection.class); Statement stmt = conn.createStatement(); stmt.execute(\"LISTEN mymessage\"); stmt.close(); } public void run() { try { while (true) { org.postgresql.PGNotification notifications[] = pgconn.getNotifications(); // If this thread is the only one that uses the connection, a timeout can be used to  // receive notifications immediately:  // org.postgresql.PGNotification notifications[] = pgconn.getNotifications(10000);  if (notifications != null) { for (int i = 0; i \u003c notifications.length; i++) System.out.println(\"Got notification: \" + notifications[i].getName()); } // wait a while before checking again for new  // notifications  Thread.sleep(500); } } catch (SQLException sqle) { sqle.printStackTrace(); } catch (InterruptedException ie) { ie.printStackTrace(); } } } class Notifier extends Thread { private Connection conn; public Notifier(Connection conn) { this.conn = conn; } public void run() { while (true) { try { Statement stmt = conn.createStatement(); stmt.execute(\"NOTIFY mymessage\"); stmt.close(); Thread.sleep(2000); } catch (SQLException sqle) { sqle.printStackTrace(); } catch (InterruptedException ie) { ie.printStackTrace(); } } } } Server Prepared Statements    Motivation    The PostgreSQL™ server allows clients to compile sql statements that are expected to be reused to avoid the overhead of parsing and planning the statement for every execution. This functionality is available at the SQL level via PREPARE and EXECUTE beginning with server version 7.3, and at the protocol level beginning with server version 7.4, but as Java developers we really just want to use the standard PreparedStatement interface.\n NOTE\nPostgreSQL 9.2 release notes: prepared statements used to be optimized once, without any knowledgeof the parameters’ values. With 9.2, the planner will use specific plans regarding to the parameters sent (the query will be planned at execution), except if the query is executed several times and the planner decides that the generic plan is not too much more expensive than the specific plans.\n Server side prepared statements can improve execution speed as\n It sends just statement handle (e.g. S_1) instead of full SQL text It enables use of binary transfer (e.g. binary int4, binary timestamps, etc); the parameters and results are much faster to parse It enables the reuse server-side execution plan The client can reuse result set column definition, so it does not have to receive and parse metadata on each execution  Activation    Previous versions of the driver used PREPARE and EXECUTE to implement server-prepared statements. This is supported on all server versions beginning with 7.3, but produced application-visible changes in query results, such as missing ResultSet metadata and row update counts. The current driver uses the V3 protocol-level equivalents which avoid these changes in query results.\nThe driver uses server side prepared statements by default when PreparedStatement API is used. In order to get to server-side prepare, you need to execute the query 5 times (that can be configured via prepareThreshold connection property).\nAn internal counter keeps track of how many times the statement has been executed and when it reaches the threshold it will start to use server side prepared statements.\nIt is generally a good idea to reuse the same PreparedStatement object for performance reasons, however the driver is able to server-prepare statements automatically across connection.prepareStatement(...) calls.\nFor instance:\nPreparedStatement ps = con.prepareStatement(\"select /*test*/ ?::int4\"); ps.setInt(1, 42); ps.executeQuery().close(); ps.close(); PreparedStatement ps = con.prepareStatement(\"select /*test*/ ?::int4\"); ps.setInt(1, 43); ps.executeQuery().close(); ps.close(); is less efficient than\nPreparedStatement ps = con.prepareStatement(\"select /*test*/ ?::int4\"); ps.setInt(1, 42); ps.executeQuery().close(); ps.setInt(1, 43); ps.executeQuery().close(); however pgJDBC can use server side prepared statements in both cases.\n Note\nThe Statement object is bound to a Connection , and it is not a good idea to access the same Statement and/or Connection from multiple concurrent threads (except cancel() , close() , and alike cases). It might be safer to just close() the statement rather than trying to cache it somehow.\n Server-prepared statements consume memory both on the client and the server, so pgJDBC limits the number of server-prepared statements per connection. It can be configured via preparedStatementCacheQueries (default 256 , the number of queries known to pgJDBC), and preparedStatementCacheSizeMiB (default 5 , that is the client side cache size in megabytes per connection). Only a subset of statement cache is server-prepared as some of the statements might fail to reach prepareThreshold .\nDeactivation    There might be cases when you would want to disable use of server-prepared statements. For instance, if you route connections through a balancer that is incompatible with server-prepared statements, you have little choice.\nYou can disable usage of server side prepared statements by setting prepareThreshold=0\nCorner cases    DDL    V3 protocol avoids sending column metadata on each execution, and BIND message specifies output column format. That creates a problem for cases like\nSELECT*FROMmytable;ALTERmytableADDcolumn...;SELECT*FROMmytable;That results in cached plan must not change result type error, and it causes the transaction to fail.\nThe recommendation is:\n Use explicit column names in the SELECT list Avoid column type alters  DEALLOCATE ALL, DISCARD ALL    There are explicit commands to deallocate all server side prepared statements. It would result in the following server-side error message: prepared statement name is invalid. Of course it could defeat pgJDBC, however there are cases when you need to discard statements (e.g. after lots of DDLs)\nThe recommendation is:\n Use simple DEALLOCATE ALL and/or DISCARD ALL commands, avoid nesting the commands into pl/pgsql or alike. The driver does understand top-level DEALLOCATE/DISCARD commands, and it invalidates client-side cache as well Reconnect. The cache is per connection, so it would get invalidated if you reconnect  set search_path = …    PostgreSQL allows to customize search_path , and it provides great power to the developer. With great power the following case could happen:\nsetsearch_path='app_v1';SELECT*FROMmytable;setsearch_path='app_v2';SELECT*FROMmytable;-- Does mytable mean app_v1.mytable or app_v2.mytable here? Server side prepared statements are linked to database object IDs, so it could fetch data from “old” app_v1.mytable table. It is hard to tell which behaviour is expected, however pgJDBC tries to track search_path changes, and it invalidates prepare cache accordingly.\nThe recommendation is:\n Avoid changing search_path often, as it invalidates server side prepared statements Use simple set search_path... commands, avoid nesting the commands into pl/pgsql or alike, otherwise pgJDBC won’t be able to identify search_path change  Re-execution of failed statements    It is a pity that a single cached plan must not change result type could cause the whole transaction to fail. The driver could re-execute the statement automatically in certain cases.\n In case the transaction has not failed (e.g. the transaction did not exist before execution of the statement that caused cached plan... error), then pgJDBC re-executes the statement automatically. This makes the application happy, and avoids unnecessary errors. In case the transaction is in a failed state, there’s nothing to do but rollback it. pgJDBC does have “automatic savepoint” feature, and it could automatically rollback and retry the statement. The behaviour is controlled via autosave property (default never ). The value of conservative would auto-rollback for the errors related to invalid server-prepared statements.   Note\nautosave might result in severe performance issues for long transactions, as PostgreSQL backend is not optimized for the case of long transactions and lots of savepoints.\n Replication connection    PostgreSQL replication connection does not allow to use server side prepared statements, so pgJDBC uses simple queries in the case where replication connection property is activated.\nUse of server-prepared statements for con.createStatement()    By default, pgJDBC uses server-prepared statements for PreparedStatement only, however you might want to activate server side prepared statements for regular Statement as well. For instance, if you execute the same statement through con.createStatement().executeQuery(...) , then you might improve performance by caching the statement. Of course it is better to use PreparedStatements explicitly, however the driver has an option to cache simple statements as well.\nYou can do that by setting preferQueryMode to extendedCacheEverything.\n Note\nthe option is more of a diagnostinc/debugging sort, so be careful how you use it .\n Bind placeholder datatypes    The database optimizes the execution plan for given parameter types. Consider the below case:\n-- create table rooms (id int4, name varchar); -- create index name__rooms on rooms(name); PreparedStatementps=con.prepareStatement(\"select id from rooms where name=?\");ps.setString(1,\"42\");It works as expected, however what would happen if one uses setInt instead? ps.setInt(1, 42);\nEven though the result would be identical, the first variation ( setString case) enables the database to use index name__rooms , and the latter does not. In case the database gets 42 as integer, it uses the plan like where cast(name as int4) = ? .\nThe plan has to be specific for the ( SQL text ; parameter types ) combination, so the driver has to invalidate server side prepared statements in case the statement is used with different parameter types.\nThis gets especially painful for batch operations as you don’t want to interrupt the batch by using alternating datatypes.\nThe most typical case is as follows (don’t ever use this in production):\nPreparedStatement ps = con.prepareStatement(\"select id from rooms where ...\"); if (param instanceof String) { ps.setString(1, param); } else if (param instanceof Integer) { ps.setInt(1, ((Integer) param).intValue()); } else { // Does it really matter which type of NULL to use?  // In fact, it does since data types specify which server-procedure to call  ps.setNull(1, Types.INTEGER); } As you might guess, setString vs setNull(..., Types.INTEGER) result in alternating datatypes, and it forces the driver to invalidate and re-prepare server side statement.\nRecommendation is to use the consistent datatype for each bind placeholder, and use the same type for setNull . Check out org.postgresql.test.jdbc2.PreparedStatementTest.testAlternatingBindType example for more details.\nDebugging    In case you run into cached plan must not change result type or prepared statement \\\"S_2\\\" does not exist the following might be helpful to debug the case.\n Client logging. If you add loggerLevel=TRACE\u0026loggerFile=pgjdbc-trace.log, you would get trace of the messages send between the driver and the backend You might check org.postgresql.test.jdbc2.AutoRollbackTestSuite as it verifies lots of combinations  Example 9.3. Using server side prepared statements    import java.sql.*; public class ServerSidePreparedStatement { public static void main(String args[]) throws Exception { Class.forName(\"org.postgresql.Driver\"); String url = \"jdbc:postgresql://localhost:5432/test\"; Connection conn = DriverManager.getConnection(url, \"test\", \"\"); PreparedStatement pstmt = conn.prepareStatement(\"SELECT ?\"); // cast to the pg extension interface  org.postgresql.PGStatement pgstmt = pstmt.unwrap(org.postgresql.PGStatement.class); // on the third execution start using server side statements  pgstmt.setPrepareThreshold(3); for (int i = 1; i \u003c= 5; i++) { pstmt.setInt(1, i); boolean usingServerPrepare = pgstmt.isUseServerPrepare(); ResultSet rs = pstmt.executeQuery(); rs.next(); System.out.println(\"Execution: \" + i + \", Used server side: \" + usingServerPrepare + \", Result: \" + rs.getInt(1)); rs.close(); } pstmt.close(); conn.close(); } } Which produces the expected result of using server side prepared statements upon the third execution.\n   Execution Used server side Result     1 false 1   2 false 2   3 true 3   4 true 4   5 true 5    The example shown above requires the programmer to use PostgreSQL™ specific code in a supposedly portable API which is not ideal. Also it sets the threshold only for that particular statement which is some extra typing if we wanted to use that threshold for every statement. Let’s take a look at the other ways to set the threshold to enable server side prepared statements. There is already a hierarchy in place above a PreparedStatement , the Connection it was created from, and above that the source of the connection be it a Datasource or a URL. The server side prepared statement threshold can be set at any of these levels such that the value will be the default for all of it’s children.\n// pg extension interfaces org.postgresql.PGConnection pgconn; org.postgresql.PGStatement pgstmt; // set a prepared statement threshold for connections created from this url String url = \"jdbc:postgresql://localhost:5432/test?prepareThreshold=3\"; // see that the connection has picked up the correct threshold from the url Connection conn = DriverManager.getConnection(url, \"test\", \"\"); pgconn = conn.unwrap(org.postgresql.PGConnection.class); System.out.println(pgconn.getPrepareThreshold()); // Should be 3  // see that the statement has picked up the correct threshold from the connection PreparedStatement pstmt = conn.prepareStatement(\"SELECT ?\"); pgstmt = pstmt.unwrap(org.postgresql.PGStatement.class); System.out.println(pgstmt.getPrepareThreshold()); // Should be 3  // change the connection's threshold and ensure that new statements pick it up pgconn.setPrepareThreshold(5); PreparedStatement pstmt = conn.prepareStatement(\"SELECT ?\"); pgstmt = pstmt.unwrap(org.postgresql.PGStatement.class); System.out.println(pgstmt.getPrepareThreshold()); // Should be 5 Parameter Status Messages    PostgreSQL supports server parameters, also called server variables or, internally, Grand Unified Configuration (GUC) variables. These variables are manipulated by the SET command, postgresql.conf , ALTER SYSTEM SET , ALTER USER SET, ALTER DATABASE SET, the set_config(...) SQL-callable function, etc. See The PostgreSQL manual.\nFor a subset of these variables the server will automatically report changes to the value to the client driver and application. These variables are known internally as GUC_REPORT variables after the name of the flag that enables the functionality.\nThe server keeps track of all the variable scopes and reports when a variable reverts to a prior value, so the client doesn’t have to guess what the current value is and whether some server-side function could’ve changed it. Whenever the value changes, no matter why or how it changes, the server reports the new effective value in a Parameter Status protocol message to the client. pgJDBC uses many of these reports internally.\nAs of pgJDBC 42.2.6, it also exposes the parameter status information to user applications via the PGConnection extensions interface.\nMethods    Two methods on org.postgresql.PGConnection provide the client interface to reported parameters. Parameter names are case-insensitive and case-preserving.\n  Map PGConnection.getParameterStatuses() - return a map of all reported parameters and their values.\n  String PGConnection.getParameterStatus() - shorthand to retrieve one value by name, or null if no value has been reported.\n  See the PGConnection JavaDoc for details.\nExample    If you’re working directly with a java.sql.Connection you can\nimport org.postgresql.PGConnection; void my_function(Connection conn) { System.out.println(\"My application name is \" + ((PGConnection) conn).getParameterStatus(\"application_name\")); } Other client drivers    The libpq equivalent is the PQparameterStatus(...) API function.\nPhysical and Logical replication API    Postgres 9.4 (released in December 2014) introduced a new feature called logical replication. Logical replication allows changes from a database to be streamed in real-time to an external system. The difference between physical replication and logical replication is that logical replication sends data over in a logical format whereas physical replication sends data over in a binary format. Additionally logical replication can send over a single table, or database. Binary replication replicates the entire cluster in an all or nothing fashion; which is to say there is no way to get a specific table or database using binary replication\nPrior to logical replication keeping an external system synchronized in real time was problematic. The application would have to update/invalidate the appropriate cache entries, reindex the data in your search engine, send it to your analytics system, and so on.\nThis suffers from race conditions and reliability problems. For example if slightly different data gets written to two different datastores (perhaps due to a bug or a race condition), the contents of the datastores will gradually drift apart — they will become more and more inconsistent over time. Recovering from such gradual data corruption is difficult.\nLogical decoding takes the database’s write-ahead log (WAL), and gives us access to row-level change events: every time a row in a table is inserted, updated or deleted, that’s an event. Those events are grouped by transaction, and appear in the order in which they were committed to the database. Aborted/rolled-back transactions do not appear in the stream. Thus, if you apply the change events in the same order, you end up with an exact, transactionally consistent copy of the database. It’s looks like the Event Sourcing pattern that you previously implemented in your application, but now it’s available out of the box from the PostgreSQL database.\nFor access to real-time changes PostgreSQL provides the streaming replication protocol. Replication protocol can be physical or logical. Physical replication protocol is used for Master/Secondary replication. Logical replication protocol can be used to stream changes to an external system.\nSince the JDBC API does not include replication PGConnection implements the PostgreSQL API\nConfigure database    Your database should be configured to enable logical or physical replication\npostgresql.conf     Property max_wal_senders should be at least equal to the number of replication consumers Property wal_keep_segments should contain count wal segments that can’t be removed from database. Property wal_level for logical replication should be equal to logical. Property max_replication_slots should be greater than zero for logical replication, because logical replication can’t work without replication slot.  pg_hba.conf    Enable connect user with replication privileges to replication stream.\nlocalreplicationalltrusthostreplicationall127.0.0.1/32md5hostreplicationall::1/128md5Configuration for examples    postgresql.conf\nmax_wal_senders = 4 # max number of walsender processes wal_keep_segments = 4 # in logfile segments, 16MB each; 0 disables wal_level = logical # minimal, replica, or logical max_replication_slots = 4 # max number of replication slots pg_hba.conf\n#Allowreplicationconnectionsfromlocalhost,byauserwiththe#replicationprivilege.localreplicationalltrusthostreplicationall127.0.0.1/32md5hostreplicationall::1/128md5Logical replication    Logical replication uses a replication slot to reserve WAL logs on the server and also defines which decoding plugin to use to decode the WAL logs to the required format, for example you can decode changes as json, protobuf, etc. To demonstrate how to use the pgJDBC replication API we will use the test_decoding plugin that is include in the postgresql-contrib package, but you can use your own decoding plugin. There are a few on github which can be used as examples.\nIn order to use the replication API, the Connection has to be created in replication mode, in this mode the connection is not available to execute SQL commands, and can only be used with replication API. This is a restriction imposed by PostgreSQL.\nExample 9.4. Create replication connection.    String url = \"jdbc:postgresql://localhost:5432/postgres\"; Properties props = new Properties(); PGProperty.USER.set(props, \"postgres\"); PGProperty.PASSWORD.set(props, \"postgres\"); PGProperty.ASSUME_MIN_SERVER_VERSION.set(props, \"9.4\"); PGProperty.REPLICATION.set(props, \"database\"); PGProperty.PREFER_QUERY_MODE.set(props, \"simple\"); Connection con = DriverManager.getConnection(url, props); PGConnection replConnection = con.unwrap(PGConnection.class); The entire replication API is grouped in org.postgresql.replication.PGReplicationConnection and is available via org.postgresql.PGConnection#getReplicationAPI .\nBefore you can start replication protocol, you need to have replication slot, which can be also created via pgJDBC API.\nExample 9.5. Create replication slot via pgJDBC API    replConnection.getReplicationAPI() .createReplicationSlot() .logical() .withSlotName(\"demo_logical_slot\") .withOutputPlugin(\"test_decoding\") .make(); Once we have the replication slot, we can create a ReplicationStream.\nExample 9.6. Create logical replication stream.    PGReplicationStream stream = replConnection.getReplicationAPI() .replicationStream() .logical() .withSlotName(\"demo_logical_slot\") .withSlotOption(\"include-xids\", false) .withSlotOption(\"skip-empty-xacts\", true) .start(); The replication stream will send all changes since the creation of the replication slot or from replication slot restart LSN if the slot was already used for replication. You can also start streaming changes from a particular LSN position, in that case LSN position should be specified when you create the replication stream.\nExample 9.7. Create logical replication stream from particular position.    LogSequenceNumber waitLSN = LogSequenceNumber.valueOf(\"6F/E3C53568\"); PGReplicationStream stream = replConnection.getReplicationAPI() .replicationStream() .logical() .withSlotName(\"demo_logical_slot\") .withSlotOption(\"include-xids\", false) .withSlotOption(\"skip-empty-xacts\", true) .withStartPosition(waitLSN) .start(); Via withSlotOption we also can specify options that will be sent to our output plugin, this allows customize decoding. For example I have my own output plugin that has a property sensitive=true which will include changes by sensitive columns to change event.\nExample 9.8. Example output with include-xids=true    BEGIN105779tablepublic.test_logic_table:INSERT:pk[integer]:1name[charactervarying]:'previous value'COMMIT105779Example 9.9. Example output with include-xids=false    BEGINtablepublic.test_logic_table:INSERT:pk[integer]:1name[charactervarying]:'previous value'COMMITDuring replication the database and consumer periodically exchange ping messages. When the database or client do not receive ping message within the configured timeout, replication has been deemed to have stopped and an exception will be thrown and the database will free resources. In PostgreSQL the ping timeout is configured by the property wal_sender_timeout (default = 60 seconds). Replication stream in pgjdc can be configured to send feedback(ping) when required or by time interval. It is recommended to send feedback(ping) to the database more often than configured wal_sender_timeout . In production I use value equal to wal_sender_timeout / 3 . It’s avoids a potential problems with networks and changes to be streamed without disconnects by timeout. To specify the feedback interval use withStatusInterval method.\nExample 9.10. Replication stream with configured feedback interval equal to 20 sec    PGReplicationStream stream = replConnection.getReplicationAPI() .replicationStream() .logical() .withSlotName(\"demo_logical_slot\") .withSlotOption(\"include-xids\", false) .withSlotOption(\"skip-empty-xacts\", true) .withStatusInterval(20, TimeUnit.SECONDS) .start(); After create PGReplicationStream , it’s time to start receive changes in real-time.\nChanges can be received from stream as blocking( org.postgresql.replication.PGReplicationStream#read ) or as non-blocking (org.postgresql.replication.PGReplicationStream#readPending ). Both methods receive changes as a java.nio.ByteBuffer with the payload from the send output plugin. We can’t receive part of message, only the full message that was sent by the output plugin. ByteBuffer contains message in format that is defined by the decoding output plugin, it can be simple String, json, or whatever the plugin determines. That why pgJDBC returns the raw ByteBuffer instead of making assumptions.\nExample 9.11. Example send message from output plugin.    OutputPluginPrepareWrite(ctx, true); appendStringInfo(ctx-\u003eout, \"BEGIN %u\", txn-\u003exid); OutputPluginWrite(ctx, true); Example 9.12. Receive changes via replication stream.\nwhile (true) { //non blocking receive message  ByteBuffer msg = stream.readPending(); if (msg == null) { TimeUnit.MILLISECONDS.sleep(10 L); continue; } int offset = msg.arrayOffset(); byte[] source = msg.array(); int length = source.length - offset; System.out.println(new String(source, offset, length)); } As mentioned previously, replication stream should periodically send feedback to the database to prevent disconnect via timeout. Feedback is automatically sent when read or readPending are called if it’s time to send feedback. Feedback can also be sent via org.postgresql.replication.PGReplicationStream#forceUpdateStatus() regardless of the timeout. Another important duty of feedback is to provide the server with the Logial Sequence Number (LSN) that has been successfully received and applied to consumer, it is necessary for monitoring and to truncate/archive WAL’s that that are no longer needed. In the event that replication has been restarted, it’s will start from last successfully processed LSN that was sent via feedback to database.\nThe API provides the following feedback mechanism to indicate the successfully applied LSN by the current consumer. LSN’s before this can be truncated or archived. org.postgresql.replication.PGReplicationStream#setFlushedLSN and org.postgresql.replication.PGReplicationStream#setAppliedLSN . You always can get last receive LSN via org.postgresql.replication.PGReplicationStream#getLastReceiveLSN .\nExample 9.13. Add feedback indicating a successfully process LSN    while (true) { //Receive last successfully send to queue message. LSN ordered.  LogSequenceNumber successfullySendToQueue = getQueueFeedback(); if (successfullySendToQueue != null) { stream.setAppliedLSN(successfullySendToQueue); stream.setFlushedLSN(successfullySendToQueue); } //non blocking receive message  ByteBuffer msg = stream.readPending(); if (msg == null) { TimeUnit.MILLISECONDS.sleep(10 L); continue; } asyncSendToQueue(msg, stream.getLastReceiveLSN()); } Example 9.14. Full example of logical replication    String url = \"jdbc:postgresql://localhost:5432/test\"; Properties props = new Properties(); PGProperty.USER.set(props, \"postgres\"); PGProperty.PASSWORD.set(props, \"postgres\"); PGProperty.ASSUME_MIN_SERVER_VERSION.set(props, \"9.4\"); PGProperty.REPLICATION.set(props, \"database\"); PGProperty.PREFER_QUERY_MODE.set(props, \"simple\"); Connection con = DriverManager.getConnection(url, props); PGConnection replConnection = con.unwrap(PGConnection.class); replConnection.getReplicationAPI() .createReplicationSlot() .logical() .withSlotName(\"demo_logical_slot\") .withOutputPlugin(\"test_decoding\") .make(); //some changes after create replication slot to demonstrate receive it sqlConnection.setAutoCommit(true); Statement st = sqlConnection.createStatement(); st.execute(\"insert into test_logic_table(name) values('first tx changes')\"); st.close(); st = sqlConnection.createStatement(); st.execute(\"update test_logic_table set name = 'second tx change' where pk = 1\"); st.close(); st = sqlConnection.createStatement(); st.execute(\"delete from test_logic_table where pk = 1\"); st.close(); PGReplicationStream stream = replConnection.getReplicationAPI() .replicationStream() .logical() .withSlotName(\"demo_logical_slot\") .withSlotOption(\"include-xids\", false) .withSlotOption(\"skip-empty-xacts\", true) .withStatusInterval(20, TimeUnit.SECONDS) .start(); while (true) { //non blocking receive message  ByteBuffer msg = stream.readPending(); if (msg == null) { TimeUnit.MILLISECONDS.sleep(10 L); continue; } int offset = msg.arrayOffset(); byte[] source = msg.array(); int length = source.length - offset; System.out.println(new String(source, offset, length)); //feedback  stream.setAppliedLSN(stream.getLastReceiveLSN()); stream.setFlushedLSN(stream.getLastReceiveLSN()); } Where output looks like this, where each line is a separate message.\nBEGINtablepublic.test_logic_table:INSERT:pk[integer]:1name[charactervarying]:'first tx changes'COMMITBEGINtablepublic.test_logic_table:UPDATE:pk[integer]:1name[charactervarying]:'second tx change'COMMITBEGINtablepublic.test_logic_table:DELETE:pk[integer]:1COMMITPhysical replication    API for physical replication looks like the API for logical replication. Physical replication does not require a replication slot. And ByteBuffer will contain the binary form of WAL logs. The binary WAL format is a very low level API, and can change from version to version. That is why replication between different major PostgreSQL versions is not possible. But physical replication can contain many important data, that is not available via logical replication. That is why pgjdc contains an implementation for both.\nExample 9.15. Use physical replication\nLogSequenceNumber lsn = getCurrentLSN(); Statement st = sqlConnection.createStatement(); st.execute(\"insert into test_physic_table(name) values('previous value')\"); st.close(); PGReplicationStream stream = pgConnection .getReplicationAPI() .replicationStream() .physical() .withStartPosition(lsn) .start(); ByteBuffer read = stream.read(); Arrays    PostgreSQL™ provides robust support for array data types as column types, function arguments and criteria in where clauses. There are several ways to create arrays with pgJDBC.\nThe java.sql. Connection.createArrayOf(String, Object[]) can be used to create an java.sql. Array from Object[] instances (Note: this includes both primitive and object multi-dimensional arrays). A similar method org.postgresql.PGConnection.createArrayOf(String, Object) provides support for primitive array types. The java.sql.Array object returned from these methods can be used in other methods, such as PreparedStatement.setArray(int, Array).\nThe following types of arrays support binary representation in requests and can be used in PreparedStatement.setObject\n   Java Type Supported binary PostgreSQL™ Types Default PostgreSQL™ Type     short[] , Short[] int2[] int2[]   int[] , Integer[] int4[] int4[]   long[] , Long[] int8[] int8[]   float[] , Float[] float4[] float4[]   double[] , Double[] float8[] float8[]   boolean[] , Boolean[] bool[] bool[]   String[] varchar[] , text[] varchar[]   byte[][] bytea[] bytea[]    ","description":"","title":"PostgreSQL™ Extensions to the JDBC API","uri":"/pgjdbc/documentation/server-prepare/"},{"categories":null,"content":"The PostgreSQL™ JDBC driver is not thread safe. The PostgreSQL server is not threaded. Each connection creates a new process on the server as such any concurrent requests to the process would have to be serialized. The driver makes no guarantees that methods on connections are synchronized. It will be up to the caller to synchronize calls to the driver.\nA notable exception is org/postgresql/jdbc/TimestampUtils.java which is threadsafe.\n","description":"","title":"Using the Driver in a Multithreaded or a Servlet Environment","uri":"/pgjdbc/documentation/thread/"},{"categories":null,"content":"JDBC 2 introduced standard connection pooling features in an add-on API known as the JDBC 2.0 Optional Package (also known as the JDBC 2.0 Standard Extension). These features have since been included in the core JDBC 3 API.\nThe JDBC API provides a client and a server interface for connection pooling. The client interface is javax.sql.DataSource , which is what application code will typically use to acquire a pooled database connection. The server interface is javax.sql.ConnectionPoolDataSource , which is how most application servers will interface with the PostgreSQL™ JDBC driver.\nIn an application server environment, the application server configuration will typically refer to the PostgreSQL™ ConnectionPoolDataSource implementation, while the application component code will typically acquire a DataSource implementation provided by the application server (not by PostgreSQL™).\nFor an environment without an application server, PostgreSQL™ provides two implementations of DataSource which an application can use directly. One implementation performs connection pooling, while the other simply provides access to database connections through the DataSource interface without any pooling. Again, these implementations should not be used in an application server environment unless the application server does not support the ConnectionPoolDataSource interface.\nApplication Servers ConnectionPoolDataSource    PostgreSQL™ includes one implementation of ConnectionPoolDataSource named org.postgresql.ds.PGConnectionPoolDataSource .\nJDBC requires that a ConnectionPoolDataSource be configured via JavaBean properties, shown in Table 11.1, “ConnectionPoolDataSource Configuration Properties”, so there are get and set methods for each of these properties.\nTable 11.1. ConnectionPoolDataSource Configuration Properties       Property Type Description     serverName STRING PostgreSQL™ database server host name   databaseName STRING PostgreSQL™ database name   portNumber INT TCP port which the PostgreSQL™ database server is listening on (or 0 to use the default port)   user STRING User used to make database connections   password STRING Password used to make database connections   ssl BOOLEAN If true , use SSL encrypted connections (default false )   sslfactory STRING Custom javax.net.ssl.SSLSocketFactory class name (see the section called [“Custom   SSLSocketFactory”](ssl-factory.html))     defaultAutoCommit BOOLEAN Whether connections should have autocommit enabled or disabled when they are supplied to the caller. The default is false , to disable autocommit.    Many application servers use a properties-style syntax to configure these properties, so it would not be unusual to enter properties as a block of text. If the application server provides a single area to enter all the properties, they might be listed like this:\nserverName=localhost\ndatabaseName=test\nuser=testuser\npassword=testpassword\nOr, if semicolons are used as separators instead of newlines, it could look like this:\nserverName=localhost;databaseName=test;user=testuser;password=testpassword\nApplications DataSource    PostgreSQL™ includes two implementations of DataSource , as shown in Table 11.2, “DataSource Implementations”.\nOne that does pooling and the other that does not. The pooling implementation does not actually close connections when the client calls the close method, but instead returns the connections to a pool of available connections for other clients to use. This avoids any overhead of repeatedly opening and closing connections, and allows a large number of clients to share a small number of database connections.\nThe pooling data-source implementation provided here is not the most feature-rich in the world. Among other things, connections are never closed until the pool itself is closed; there is no way to shrink the pool. As well, connections requested for users other than the default configured user are not pooled. Its error handling sometimes cannot remove a broken connection from the pool. In general it is not recommended to use the PostgreSQL™ provided connection pool. Check your application server or check out the excellent jakarta commons DBCP project.\nTable 11.2. DataSource Implementations       Pooling Implementation Class     No `org.postgresql.ds. PGSimpleDataSource   Yes `org.postgresql.ds. PGPoolingDataSource    Both implementations use the same configuration scheme. JDBC requires that a DataSource be configured via JavaBean properties, shown in Table 11.3, “DataSource Configuration Properties”, so there are get and set methods for each of these properties.\nTable 11.3. DataSource Configuration Properties       Property Type Description     serverName STRING PostgreSQL™ database server host name   databaseName STRING PostgreSQL™ database name   portNumber INT TCP port which the PostgreSQL™ database server is listening on (or 0 to use the default port)   user STRING User used to make database connections   password STRING Password used to make database connections   ssl BOOLEAN If true, use SSL encrypted connections (default false)   sslfactory STRING Custom javax.net.ssl. SSLSocketFactory class name (see the section called “Custom SSLSocketFactory”)    The pooling implementation requires some additional configuration properties, which are shown in Table 11.4, “Additional Pooling DataSource Configuration Properties.\nTable 11.4. Additional Pooling DataSource Configuration Properties       Property Type Description     dataSourceName STRING Every pooling DataSource must have a unique name.   initialConnections INT The number of database connections to be created when the pool is initialized.   maxConnections INT The maximum number of open database connections to allow. When more connections are requested, the caller will hang until a connection is returned to the pool.    Example 11.1, “DataSource Code Example” shows an example of typical application code using a pooling DataSource.\nExample 11.1. DataSource Code Example    Code to initialize a pooling DataSource might look like this:\nPGPoolingDataSource source = new PGPoolingDataSource(); source.setDataSourceName(\"A Data Source\"); source.setServerNames(new String[] { \"localhost\" }); source.setDatabaseName(\"test\"); source.setUser(\"testuser\"); source.setPassword(\"testpassword\"); source.setMaxConnections(10);  Note\nsetServerName has been deprecated in favour of setServerNames. This was done to support multiple hosts.\n Then code to use a connection from the pool might look like this.\n Note\nit is critical that the connections are eventually closed. Else the pool will “leak” connections and will eventually lock all the clients out.\n try (Connection conn = source.getConnection()) { // use connection } catch (SQLException e) { // log error } Data Sources and JNDI    All the ConnectionPoolDataSource and DataSource implementations can be stored in JNDI. In the case of the nonpooling implementations, a new instance will be created every time the object is retrieved from JNDI, with the same settings as the instance that was stored. For the pooling implementations, the same instance will be retrieved as long as it is available (e.g., not a different JVM retrieving the pool from JNDI), or a new instance with the same settings created otherwise.\nIn the application server environment, typically the application server’s DataSource instance will be stored in JNDI, instead of the PostgreSQL™ ConnectionPoolDataSource implementation.\nIn an application environment, the application may store the DataSource in JNDI so that it doesn’t have to make a reference to the DataSource available to all application components that may need to use it. An example of this is shown in Example 11.2, “DataSource JNDI Code Example”.\nExample 11.2. DataSource JNDI Code Example    Application code to initialize a pooling DataSource and add it to JNDI might look like this:\nPGPoolingDataSource source = new PGPoolingDataSource(); source.setDataSourceName(\"A Data Source\"); source.setServerName(\"localhost\"); source.setDatabaseName(\"test\"); source.setUser(\"testuser\"); source.setPassword(\"testpassword\"); source.setMaxConnections(10); new InitialContext().rebind(\"DataSource\", source); Then code to use a connection from the pool might look like this:\nConnection conn = null; try { DataSource source = (DataSource) new InitialContext().lookup(\"DataSource\"); conn = source.getConnection(); // use connection } catch (SQLException e) { // log error } catch (NamingException e) { // DataSource wasn't found in JNDI } finally { if (con != null) { try { conn.close(); } catch (SQLException e) {} } } Tomcat setup     NOTE\nThe postgresql.jar file must be placed in $CATALINA_HOME/common/lib in both Tomcat 4 and 5.\n The absolute easiest way to set this up in either tomcat instance is to use the admin web application that comes with Tomcat, simply add the datasource to the context you want to use it in.\nSetup for Tomcat 4 place the following inside the \u003c Context\u003e tag inside conf/server.xml\n\u003cResource name=\"jdbc/postgres\" scope=\"Shareable\" type=\"javax.sql.DataSource\"/\u003e \u003cResourceParams name=\"jdbc/postgres\"\u003e \u003cparameter\u003e \u003cname\u003evalidationQuery\u003c/name\u003e \u003cvalue\u003eselect version();\u003c/value\u003e \u003c/parameter\u003e \u003cparameter\u003e \u003cname\u003eurl\u003c/name\u003e \u003cvalue\u003ejdbc:postgresql://localhost/davec\u003c/value\u003e \u003c/parameter\u003e \u003cparameter\u003e \u003cname\u003epassword\u003c/name\u003e \u003cvalue\u003edavec\u003c/value\u003e \u003c/parameter\u003e \u003cparameter\u003e \u003cname\u003emaxActive\u003c/name\u003e \u003cvalue\u003e4\u003c/value\u003e \u003c/parameter\u003e \u003cparameter\u003e \u003cname\u003emaxWait\u003c/name\u003e \u003cvalue\u003e5000\u003c/value\u003e \u003c/parameter\u003e \u003cparameter\u003e \u003cname\u003edriverClassName\u003c/name\u003e \u003cvalue\u003eorg.postgresql.Driver\u003c/value\u003e \u003c/parameter\u003e \u003cparameter\u003e \u003cname\u003eusername\u003c/name\u003e \u003cvalue\u003edavec\u003c/value\u003e \u003c/parameter\u003e \u003cparameter\u003e \u003cname\u003emaxIdle\u003c/name\u003e \u003cvalue\u003e2\u003c/value\u003e \u003c/parameter\u003e \u003c/ResourceParams\u003e Setup for Tomcat 5, you can use the above method, except that it goes inside the \u003c DefaultContext\u003e tag inside the \u003c Host\u003e tag. eg. \u003c Host\u003e … \u003c DefaultContext\u003e …\nAlternatively there is a conf/Catalina/hostname/context.xml file. For example http://localhost:8080/servlet-example has a directory $CATALINA_HOME/conf/Catalina/localhost/servlet-example.xml file. Inside this file place the above xml inside the \u003c Context\u003e tag\nThen you can use the following code to access the connection.\nimport javax.naming.*; import javax.sql.*; import java.sql.*; public class DBTest { String foo = \"Not Connected\"; int bar = -1; public void init() { try { Context ctx = new InitialContext(); if (ctx == null) throw new Exception(\"Boom - No Context\"); // /jdbc/postgres is the name of the resource above  DataSource ds = (DataSource) ctx.lookup(\"java:comp/env/jdbc/postgres\"); if (ds != null) { Connection conn = ds.getConnection(); if (conn != null) { foo = \"Got Connection \" + conn.toString(); Statement stmt = conn.createStatement(); ResultSet rst = stmt.executeQuery(\"select id, foo, bar from testdata\"); if (rst.next()) { foo = rst.getString(2); bar = rst.getInt(3); } conn.close(); } } } catch (Exception e) { e.printStackTrace(); } } public String getFoo() { return foo; } public int getBar() { return bar; } } ","description":"","title":"Connection Pools and Data Sources","uri":"/pgjdbc/documentation/datasource/"},{"categories":null,"content":"The PostgreSQL JDBC Driver supports the use of logging (or tracing) to help resolve issues with the pgJDBC Driver when is used in your application.\nThe pgJDBC Driver uses the logging APIs of java.util.logging that is part of Java since JDK 1.4, which makes it a good choice for the driver since it doesn’t add any external dependency for a logging framework. java.util.logging is a very rich and powerful tool, it’s beyond the scope of these docs to explain how to use it to it’s full potential, for that please refer to Java Logging Overview.\nThis logging support was added since version 42.0.0 of the pgJDBC Driver, and previous versions uses a custom mechanism to enable logging that it is replaced by the use of java.util.logging in current versions, the old mechanism is no longer available.\n NOTE\nPlease note that while most people asked the use of a Logging Framework for a long time, this support is mainly to debug the driver itself and not for general sql query debug.\n Configuration    The Logging APIs offer both static and dynamic configuration control. Static control enables field service staff to set up a particular configuration and then re-launch the application with the new logging settings. Dynamic control allows for updates to the logging configuration within a currently running program.\nThe root logger used by the pgJDBC driver is org.postgresql .\nEnable logging by using logging.properties file    The default Java logging framework stores its configuration in a file called logging.properties . Settings are stored per line using a dot notation format. Java installs a global configuration file in the lib folder of the Java installation directory, although you can use a separate configuration file by specifying the java.util.logging.config.file property when starting a Java program. logging.properties files can also be created and stored with individual projects.\nThe following is an example of setting that you can make in the logging.properties :\n# Specify the handler, the handlers will be installed during VM startup. handlers = java.util.logging.FileHandler # Default global logging level. .level = OFF # Default file output is in user's home directory. java.util.logging.FileHandler.pattern = %h/pgjdbc%u.log java.util.logging.FileHandler.limit = 5000000 java.util.logging.FileHandler.count = 20 java.util.logging.FileHandler.formatter = java.util.logging.SimpleFormatter java.util.logging.FileHandler.level = FINEST java.util.logging.SimpleFormatter.format = %1$tY-%1$tm-%1$td %1$tH:%1$tM:%1$tS %4$s %2$s %5$s%6$s%n # Facility specific properties. org.postgresql.level = FINEST And when you run your application you pass the system property:\njava -jar -Djava.util.logging.config.file=logging.properties run.jar\n","description":"","title":"Logging using java.util.logging","uri":"/pgjdbc/documentation/logging/"},{"categories":null,"content":"If you have not yet read it, you are advised you read the JDBC API Documentation (supplied with Oracle’s JDK) and the JDBC Specification. Both are available from here.\nDocs contains updated information not included in this manual including Javadoc class documentation and a FAQ. Additionally it offers precompiled drivers.\n","description":"","title":"Further Reading","uri":"/pgjdbc/documentation/reading/"}]